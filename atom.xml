<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Woody Tells</title>
  
  <subtitle>Never Settle</subtitle>
  <link href="http://woody5962.github.io/atom.xml" rel="self"/>
  
  <link href="http://woody5962.github.io/"/>
  <updated>2021-07-12T04:06:15.857Z</updated>
  <id>http://woody5962.github.io/</id>
  
  <author>
    <name>Woody</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>元学习简介</title>
    <link href="http://woody5962.github.io/Meta-Learning/"/>
    <id>http://woody5962.github.io/Meta-Learning/</id>
    <published>2021-03-09T16:00:00.000Z</published>
    <updated>2021-07-12T04:06:15.857Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>说白了，就是学习一个meta- knowledge，使用这个knowledge去快速适应新的任务。</p><p>即，学习怎么去学习。</p><h2 id="与传统机器学习的区别">与传统机器学习的区别</h2><p>这里学习的目标是一个meta-knowledge，feed的不是数据而是任务，分为train task和test task，而每一个task内部有训练数据和测试数据，分别叫做support set和query set。相对应的，在测试中输出的并不是对support set的计算结果，而是一个针对测试task的f，在测试task的query set上可以得到f（data）</p><p><img src="../images/ml1.png" /></p><h2 id="与迁移学习的区别">与迁移学习的区别</h2><ol type="1"><li>这里有专门的meta-net，对不同的task会专门使用meta-net初始化一套specific task net，在task net中更新参数，然后利用更新之后的参数进行损失计算并回传更新（可以1步，可以k步，步数越少越符合我们的使用目标，就是可以使meta-net生成的模型更快的适应task），此时再对meta-net中的参数进行同方向的更新，目的是使训练过程和使用目的一致，而使用目的正是能够对新的任务生成f，f能够快速适应该任务，在之后产生不错的效果，但并不是追求当下的效果。</li></ol><p>下图分别展示了maml和reptile的meta-learning框架：</p><p><img src="../images/ml2.png" /> <img src="../images/ml3.png" /></p><ol start="2" type="1"><li>迁移学习使用同一套参数，并不会针对专门的任务另外初始化一套网络，每一个任务的损失都直接影响model param，目的是得到当下的最优，也就是针对新任务可以直接有较优的结果。</li></ol><p><img src="../images/ml4.png" /> <img src="../images/ml5.png" /></p>]]></content>
    
    
    <summary type="html">元学习简介及其与传统机器学习、迁移学习的区别</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="元学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="元学习" scheme="http://woody5962.github.io/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>经典分类算法简介</title>
    <link href="http://woody5962.github.io/%E5%B8%B8%E7%94%A8%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E5%B8%B8%E7%94%A8%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-10-09T16:00:00.000Z</published>
    <updated>2021-07-13T06:42:47.441Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典分类算法">经典分类算法</h2><p><img src="../images/fl1.png" /></p><h3 id="贝叶斯方法体系">贝叶斯方法体系</h3><p>不同的贝叶斯方法的区别在于计算后验概率时的分子，也就是联合分布概率的计算。贝叶斯网络的联合分布可以根据网络图很直观地得到结果，具体可以学习一下概率图模型中的贝叶斯网络（与马尔可夫网络的区别就在于后者具有最大团属性，对应于马尔可夫性）。</p><ol type="1"><li>朴素贝叶斯</li></ol><ul><li>假设所有特征是独立的，无相互作用，可直接通过先验概率去计算后验概率。</li></ul><ol start="2" type="1"><li>半朴素贝叶斯</li></ol><ul><li>适当考虑一部分属性间的相互依赖信息。</li><li>典型的是独依赖估计</li></ul><ol start="3" type="1"><li>贝叶斯网</li></ol><ul><li>也叫信念网（belief network）</li><li>借助有向无环图来刻画属性之间的依赖关系，使用条件概率表来描述属性的联合概率分布。</li><li>贝叶斯网络的大致原理是构造各维特征之间的依赖网络（通过某种基于信息论准则的学习过程构建），然后根据学习到的贝叶斯网络B（G，P）通过Gibbs采样来模拟类别概率。</li></ul><h2 id="knn">KNN</h2><p>最简单的一种分类思路，利用样本点周围样本的类别来做分类，唯一需要调整的参数就是所用邻居样本点的数目k。</p><p><img src="../images/fl2.png" /></p><p>另外，根据不同的数据特点确定距离度量也至关重要。</p><h3 id="常见的距离度量">常见的距离度量</h3><ul><li>Minkowski-distance</li><li>Cosine similarity</li><li>Jaccard similarity：类别向量常用的一种距离度量，具体可见<a href="../数据预处理/">数据预处理</a></li><li>KL-divergence</li></ul><h2 id="决策树">决策树</h2><p>分类使用的决策树根据不同的启发函数进行分枝，启发函数使用的一般都是信息论中的度量，比如信息增益、GINI不纯度的等等。</p><p>根据所用启发函数的区别，常用的三类决策树为：</p><ol type="1"><li>ID3: 使用信息增益</li><li>C4.5: 使用信息增益比，解决ID3对于多取值特征的倾向性。</li><li>CART：使用GINI不纯度</li></ol><h3 id="剪枝">剪枝</h3><h4 id="预剪枝">预剪枝</h4><p>在建树过程中，根据一些预定义的限制条件进行剪枝，比如树的最大深度、叶子结点的最少样本数、分枝后的增益等等。</p><p>要主要的是，预剪枝有欠拟合风险，虽然当前的划分会导测准确率降低但在之后的划分中准确率可能会有显著上升。</p><h4 id="后剪枝">后剪枝</h4><p>后剪枝是在建树完成之后，对完整的决策树进行剪枝，在实际使用中效果一般会优于预剪枝，当然也会更耗时。</p><p>其根据剪枝之后精度损失和复杂度做一个权衡，根据权衡标准选择是否剪枝。</p><p>后剪枝都会损失精度，但是为了提升模型的泛化性，我们也可以在接受一定损失的情况下去做剪枝。</p><p>比如代价复杂度剪枝CCP，思想就是看去掉之后的节点之后，模型损失的精度和精简的复杂度之间的权衡，找一个在一定精度损失的情况下，可精简复杂度最高的。</p><h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2><p><img src="../images/fl3.png" /></p><p>核心是将要求的条件概率转换为右边容易求的三个概率，尤其是分子的式子（因为分母是固定的），将其最大化，即bayes分类模型。</p><p>然后，做一个很朴素的假设，假设X的取值是条件独立的，即：</p><p><img src="../images/fl4.png" /></p><p>这样的话，可以通过MLE（maximum liklihood estimate,极大似然估计）得到贝叶斯估计的分类结果，若为连续变量，则进行离散化，依旧使用上述方法：</p><p><img src="../images/fl5.png" /></p><h2 id="svm">SVM</h2><p>基本思想：最大化类别之间的margin</p><p>算法推导思想：将最大化类别之间margin的目标函数使用拉格朗日方法进行转化，使用SMO算法（坐标上升法）对其对偶问题进行求解。</p><p>当数据线性可分时，我们直接使用线性SVM即可，否则可以通过核方法对其进行映射，得到高维空间的线性可分数据。</p><h2 id="逻辑回归">逻辑回归</h2><p>直接对分类的概率进行建模，使用MLE进行模型参数的求解即可。</p><p>最后直接就可以通过自变量x得到分类y=k的概率。</p><p><img src="../images/fl6.png" /></p><p>z就是一个回归模型，但是回归的并不是概率，而是分类几率的对数，所谓几率就是二分类中两种分类概率的比值，如下所示：</p><p><img src="../images/fl7.png" /></p><h2 id="分类模型的评估">分类模型的评估</h2><p>在分类问题中，我们将数据进行如下划分：</p><p><img src="../images/fl10.png" /></p><h3 id="precision">precision</h3><p>准确率，即所有分为positive的样本中，真正positive的比例。</p><h3 id="recall">recall</h3><p>召回率，即所有真正的positive样本中，被正确分为positive的比例。</p><h3 id="f1-score">f1 score</h3><p>准确率和召回率的调和平均数，兼顾二者的表现。</p><h3 id="混淆矩阵">混淆矩阵</h3><p><img src="../images/fl8.png" /></p><h3 id="roc和auc">ROC和AUC</h3><p>AUC：一个正例预测为正的概率值大于负例预测为正的概率。</p><p>表现在图像上就是ROC曲线下的面积。</p><p><img src="../images/fl9.png" /></p><p>图像上每一个点都对应一个阈值。随着阈值不断减小，TPR和FPR都不断增大，反之不断减小。</p><p>AUC是一种较为稳健的评估方式，尤其是在数据失衡的情况下，f1会非常容易受到样本比例的影响，比如正样本占绝大多数时，可能一个愚蠢的模型会将所有的样本划分为正样本，也不会对f1造成多大的影响，但是AUC会敏感的察觉到这种愚蠢的模型。</p>]]></content>
    
    
    <summary type="html">总结一下机器学习领域几种经典的分类算法，包括SVM、KNN、决策树、逻辑回归等等。</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类算法" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类算法" scheme="http://woody5962.github.io/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>集成学习概述</title>
    <link href="http://woody5962.github.io/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://woody5962.github.io/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-09-09T16:00:00.000Z</published>
    <updated>2021-07-06T08:09:17.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要用集成学习">为什么要用集成学习</h2><ol type="1"><li>More flexible to interpret</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132442.png" /></p><ol start="2" type="1"><li>Reduce misclassification rate</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132448.png" /></p><h2 id="两个常用的集成学习框架">两个常用的集成学习框架</h2><ol type="1"><li>Bagging<ul><li>random forests</li></ul></li><li>Boosting<ul><li>Adaboost</li><li>GBDT</li><li>XGBoost</li></ul></li></ol><h2 id="bagging">Bagging</h2><p>基本思想：通过重置抽样（有放回抽样）在给定的数据集D上抽出m个大小和D一样的数据集，分别用来训练m个分类器，然后将m个分类器的结果通过某种方式形成最终的分类结果。</p><h3 id="自助采样法">自助采样法</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132452.png" /></p><p>注意：样本集原本有n个样本，就采样n次。这样样本集中会包括63.2%的原始样本（每个样本都有63.2%的概率会在n次采样中被选中），剩下的36.8%用作对泛化性能的包外估计（out-of-bag estimate）</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132455.png" /></p><h3 id="方差分析">方差分析</h3><p>由于多个分类器参与了最终结果的生成，所以方差相对于单个分类器是有所变化的，准确来说，方差要小于单个分类器的方差。</p><p><span class="math display">\[\hat f_{bag}(x) = \frac{1}{M}\sum^M_{m=1}{\hat f_m(x)}\]</span></p><p><span class="math display">\[\begin{align}Var(\hat f_{bag}(x))=&amp; \frac{1}{M^2}[\sum^M_{m=1}Var(\hat f_m(x))+\sum_{t\neq m}Cov(\hat f_t(x),\hat f_m())]\\=&amp; \frac{1}{M^2}[M\sigma^2+M(M-1)\rho(x)\sigma^2]\\=&amp; \frac{1}{M}\sigma^2 + \frac{M-1}{M}\rho(x)\sigma^2\\=&amp; \rho(x)\sigma^2+\frac{1-\rho(x)}{M}\sigma^2\end{align}\]</span></p><p>其中，由于<span class="math inline">\(\rho(x)=\frac{Cov(\hat f_t(x),\hat f_m(x))}{\sigma_\theta \cdot \sigma_m}\)</span>，而<span class="math inline">\(\sigma_t=\sigma_m=\sigma\)</span> <span class="math display">\[\begin{align}\rho(x)=\frac{Cov(\hat f_t(x),\hat f_m(x))}{\sigma^2} \\Cov(\hat f_t(x),\hat f_m(x)) = \rho(x)\sigma^2\end{align}\]</span> 由于在<span class="math inline">\(Var(\hat f_{bag}(x))\)</span>计算式中，将<span class="math inline">\(f_t(x)\)</span>与<span class="math inline">\(f_m(x)\)</span>前后位置做了区分，故一共有<span class="math inline">\(m(m-1)\)</span>对。</p><p>在此，暂且将上面的T认为是M。</p><p>可见： 1. M越大，方差越小。 2. pearson相关系数越小，方差也越小。</p><p>所以我们在bagging中引入random sampling，就是为了减小相关系数。</p><h3 id="decision-tree-and-bagging-tree">decision tree and bagging tree</h3><p>决策树分两种：分类树和回归树</p><p>其中，分类树以信息论中的启发函数构造决策树，回归树以最小平方误差作为启发函数构造决策树。</p><p>以下是最小二乘回归树的构造算法：</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135239.png" /></p><p>其中，c1和c2分别是划分的两个区域对应的y的估计值（也就是输出值），通过最小化估计值与真实值之间的差值来构造决策树，启发信息就是这个差值。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135244.png" /> <img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135249.png" /></p><h3 id="随机森林">随机森林</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135245.png" /></p><ul><li>随机森林就是带随机采样的决策树/回归树森林。</li><li>随机性体现在采样和特征集合选择的随机性</li></ul><h3 id="结合策略">结合策略</h3><h4 id="针对数值型输出">针对数值型输出</h4><ol type="1"><li>简单平均法</li><li>加权平均法</li></ol><h4 id="针对类别型输出">针对类别型输出</h4><ol type="1"><li>绝对多数投票法（某类别超过一半的票）</li><li>相对多数投票法（某类别票数最多）</li><li>加权投票法</li></ol><h4 id="学习法">学习法</h4><p>这是一种比较独特的结合策略，通过串行学习来将不同的模型结果结合起来。</p><p>典型代表是stacking，这种结合策略和boosting颇有点神似。</p><p>基本思路是先从初始数据集中训练出初级学习器，然后基于初级学习器结果生成一个新的数据集用于训练次级学习器。</p><ul><li>将每个点的T个初级学习器结果作为输入特征，该点的原始标签作为新数据集的标签得到新数据集。</li><li>在新数据集上使用次级学习算法对次级学习器进行学习。</li></ul><h2 id="boosting">boosting</h2><h2 id="模型框架">模型框架</h2><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135259.png" /></p><h3 id="adaboost">AdaBoost</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135257.png" /></p><ul><li>推导过程见《机器学习》</li><li>其中基分类器权重α_m 是通过添加第m个分类器hm之后最小化指数损失函数得来的</li><li>理想的新分类器hm可以纠正Gm-1的全部错误，基于这样的推导得到了样本分布更新公式，在接受带权样本的算法中直接就是样本的权重，不接受带权样本就直接re-sample。</li><li>理解样本分布更新公式：将所有错分样本的权重增加，使得下一波分类器更加关注这些样本。</li><li>上述算法并不很完善，建议看以下《机器学习》的adaboosting算法及推导（实在懒得做搬运工了）。</li></ul><p>adaboost的思想就是通过不断添加分类器来纠正之前分类器的错误，这是通过在新的样本分布上使用新分类器最小化损失函数得来的。而提升树是很直观的对残差进行建模，不断添加回归树来补充残差。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135303.png" /></p><h4 id="损失函数">损失函数</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135305.png" /></p><p>Adaboost应该使用指数损失函数或者二项式对数似然损失函数（本质上也使用了指数损失），并且后者效果往往还比指数损失要好一些。</p><h5 id="关于指数损失函数">关于指数损失函数</h5><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135307.png" /></p><h3 id="gbdt">GBDT</h3><h4 id="boosting-tree">boosting tree</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135310.png" /></p><p>提升树包括梯度提升树都是通过残差拟合进行boosting的，与标准的boosting相比，权重更新体现在残差越大的样本，其在回归树中的结果就越大，只不过标准的boosting还需要将更新权重的样本重新进行训练，而提升树直接就得到了相应的叶节点区域，直接影响了结果，直接在上一轮学习器结果的基础上，加上残差进行校正，不停的迭代，学习器残差就会越来越小。</p><h4 id="算法流程">算法流程</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135312.png" /> <img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706144857.png" /></p><h4 id="算法解析">算法解析</h4><ol type="1"><li>初始化就是求一个只有一个叶节点区域的树，对于回归树而言，就是所有的样本点都拟合成一个值。</li><li>使用负梯度代替残差，这样更加通用。</li><li>生成新的数据集（xi, rmi）作为生成第m棵回归树的训练数据，得到j个叶节点区域，对每一个叶节点区域计算回归目标值，也就是得到j个回归目标值。</li><li>将j个针对残差的回归目标值乘以示性函数加到之前得到的提升树结果中。</li><li>最终的回归树用法：得到x在M棵回归树中的叶节点区域目标值，进行累加，其实就是一步一步的将残差进行补充。</li><li>提升树类算法就是通过不断向结果中添加残差来对结果进行校正。</li></ol><h4 id="正则化技巧">正则化技巧</h4><ol type="1"><li>shrinkage ：每次更新，在残差拟合结果上乘以正则化系数</li><li>下采样： 每次更新，都是在训练集的一个子集上进行的</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135319.png" /></p><h3 id="xgboost">XGBoost</h3><h4 id="损失函数-1">损失函数</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135323.png" /></p><p><span class="math display">\[L=\sum^{n}_{i=1}l(y_i,\hat y_i)+\sum_k\Omega(f_k)\]</span> 其中，第t棵树生成： <span class="math display">\[\begin{align}L^{(t)}=&amp;\sum^{n}_{i=1}l(y_i,y_i^{(t)})+\Omega(f_t)\\=&amp;\sum^{n}_{i=1}l(y_i,y_i^{(t-1)}+f_t(x_i))+\Omega(f_t)\\二阶泰勒展开\Rightarrow\ \ \simeq&amp;\sum^n_{i=1}[l(y_i,y_i^{(t-1)})+g_if_t(x_i)+\frac{h_i}{2}f^2_t(x_i)]+\Omega(f_t)\end{align}\]</span> 由于<span class="math inline">\(l(y_i,y_i^{(t-1)})\)</span>对<span class="math inline">\(f_t\)</span>的优化无影响</p><p>另<span class="math inline">\(L^(t)=\sum^n_{i-1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)\)</span></p><p>将同一叶节点区域的样本点合并 <span class="math display">\[\begin{align}L^{(t)}=&amp;\sum^T_{j=1}[(\sum_{i\in I_j}g_i)f_i(x)+\frac{1}{2}(\sum_{i\in I_j}h_i)f_j^2(x)]+\gamma T+\frac{\lambda}{2}\sum^T_{j=1}f_j^2(x)\\=&amp;\sum^T_{j=1}[(\sum_{i\in I_j}g_i)f_i(x)+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)f_j^2(x)]+\gamma T\end{align}\]</span> 另<span class="math inline">\(G_j=\sum_{i\in Ij}g_j \ \ \ \ \ H_j=\sum_{i\in I_j}h_i\)</span> <span class="math display">\[L^{(t)}=\sum^T_{j=1}[G_jf_j(x)+\frac{1}{2}(H_jj+\lambda)f_j^2(x)]+\gamma T\]</span> 构造出关于<span class="math inline">\(f_i(x)\)</span>的一元二次方程，解得 <span class="math display">\[\begin{align}f_j(x)=&amp;-\frac{G_j}{H_j+\lambda}\\\hat L^{(t)}=&amp;-\frac{1}{2}\sum^T_{j=1}\frac{G_j^2}{H_j+\lambda}+\gamma T\end{align}\]</span> 其中，<span class="math inline">\(g_i=\frac{\partial l(y_i,y_i^{t-1})}{\partial ly_i^{t-1}}\ \ \ \ \ \ \ \ h_i=\frac{\partial^2 l(y_i,y_i^{t-1})}{(\partial y_i^(t-1))^2}\)</span>已知</p><p>新的<span class="math inline">\(T\)</span>个叶子节点值可由<span class="math inline">\(f_j(x)=\frac{G_j}{H_j+\lambda}\)</span>得到。</p><ul><li>简而言之，GBDT使用最速下降法，这里使用牛顿法进行优化。</li><li>同样的，都是在已有的学习器基础上添加新的学习器以校正其效果，根据公式可以看到GBDT类似于最速下降法优化学习器，而XGBoost类似于牛顿法优化学习器，二者都是对学习器结果的直接校正，某些情况下可以认为是残差，但是准确来说，就是损失函数关于学习器的负梯度变形。</li></ul><p>ps：牛顿法</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142506.png" /></p><h4 id="分裂准则">分裂准则</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142504.png" /> <img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142501.png" /></p><p>综合以上，就是利用得到的目标函数进行启发式分支，然后得到的叶子节点的值有公式给出。</p><h4 id="缺失值的处理">缺失值的处理</h4><p>在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。</p><p>在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。</p><p>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142458.png" /> <img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142455.png" /></p><ul><li>可以看到，最外层遍历k从1到m是在遍历所有m个特征。</li><li>内层是将第k个特征所有无缺失值的样本按照特征k的值进行排序，然后遍历分裂，寻找最优分裂点。</li><li>以第一个内层for循环为例，先指定了left的G和H，然后用全集的G和L减去了Left的，所有k特征缺失的点就被划分到了right子树。</li></ul><h4 id="近似算法">近似算法</h4><p>对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。</p><p>因此XGBoost思想是对特征进行分桶，即找到l个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。</p><p>从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142453.png" /></p><h4 id="正则化">正则化</h4><p>XGBoost还提出了两种防止过拟合的方法： 1. Shrinkage 2. Column Subsampling。</p><p>Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。</p><p>Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，</p><ul><li>一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。</li><li>另一种是随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。</li></ul><p>一般情况下前者效果更好。</p><h4 id="优点">优点</h4><p>之所以XGBoost可以成为机器学习的大杀器，广泛用于数据科学竞赛和工业界，是因为它有许多优点：</p><ol type="1"><li>使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。</li><li>目标函数优化利用了损失函数关于待求函数的二阶导数</li><li>支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。</li><li>添加了对稀疏数据的处理。</li><li>交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。</li><li>支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。</li></ol><h2 id="总结">总结</h2><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142449.png" /></p><p>在这里把boosting框架分个类：</p><ul><li>adaboost是一类，这一类通过更新样本权重来训练新的分类器</li><li>还有一类是boosting tree、GBDT和xgboost，通过梯度或者残差来不断修正结果。</li></ul>]]></content>
    
    
    <summary type="html">常用的集成算法总结与分析</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="集成学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="集成学习" scheme="http://woody5962.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理</title>
    <link href="http://woody5962.github.io/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://woody5962.github.io/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</id>
    <published>2020-06-09T16:00:00.000Z</published>
    <updated>2021-07-11T16:33:41.757Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据类型">数据类型</h2><h3 id="表格数据">表格数据</h3><p>行为对象（元组、数据点、实例、样本均为相同概念），列为属性</p><h3 id="属性">属性</h3><p>维、特征、变量、属性均为相同概念</p><h3 id="数据类型-1">数据类型</h3><ul><li>名义型</li><li>布尔型</li><li>等级型</li><li>数值型</li></ul><h3 id="数据的基本统计信息">数据的基本统计信息</h3><p>统计中心趋势和离散程度：</p><ul><li>均值：算数均值、加权均值</li><li>中位数（median）</li><li>众数（mode）</li><li>对于适度倾斜的单峰曲线，有经验公式：mean - mode = 3 * (mean - median)</li></ul><p><img src="../images/cl1.png" /></p><ul><li>分位数（Quantile）：x为数据集中的数，如果数据集中百分之K的数都不大于x，则称x为数据集的K分位数</li><li>上四分位数（Q3-75% quentile）</li><li>下四分位数（Q1-25%quentile）</li><li>中间分位数（IQR ）: IQR = Q3 - Q1, 也叫四分位距，用来衡量数据的分散情况。</li><li>方差（Variance）和标准差（Standard deviation）</li></ul><p><img src="../images/cl2.png" /></p><ul><li>盒图（箱线图）</li></ul><p><img src="../images/cl3.png" /></p><h2 id="距离度量">距离度量</h2><p>相似性，值域通常为[0,1]</p><h3 id="名义型数据类型-布尔型数据类型">名义型数据类型 &amp; 布尔型数据类型</h3><ul><li>简单匹配（相同属性个数m，属性数量p，则距离为p-m/p）</li><li>转换为多个布尔属性，如下图所示</li></ul><p><img src="../images/cl4.png" /></p><p>q是两者都有的一个属性，r是i有但是j没有的属性。</p><h3 id="数值型数据类型">数值型数据类型</h3><p>明可夫斯基距离：</p><p><img src="../images/cl5.png" /> <img src="../images/cl6.png" /></p><p>前者是绝对距离，后者是方向差异。</p><p>前者体现维度数值大小的差异，后者体现类似于用户兴趣之类的差异。</p><h3 id="等级型数值类型">等级型数值类型</h3><p><img src="../images/cl7.png" /></p><h3 id="混合型数据类型">混合型数据类型</h3><p>将上述的类型求得的距离进行加权求和。</p><h2 id="数据预处理的方式">数据预处理的方式</h2><ul><li>数据清洗</li><li>数据集成</li><li>数据转换</li><li>数据归约</li></ul><p><img src="../images/cl8.png" /></p><h3 id="数据标准化">数据标准化</h3><p>算法要求或者数据量纲不同：</p><ol type="1"><li>Z-score标准化：接近（0，1）分布，适用于最值未知</li><li>0-1标准化：线性变换到[0,1]区间</li><li>小数定标标准化：</li></ol><p><img src="../images/cl9.png" /></p><ol start="4" type="1"><li>logistic标准化</li></ol><p><img src="../images/cl10.png" /></p><p>当数据集中的分布在零点附近时，通过sigmoid函数可以将其均匀地散开。</p><h4 id="方法比较">方法比较</h4><p><img src="../images/cl11.png" /></p><h3 id="数据离散化">数据离散化</h3><p>有些数据挖掘算法，特别是某些分类算法（如朴素贝叶斯），要求数据是分类属性形式（类别型属性）这样常常需要将连续属性变换成分类属性（离散化，Discretization）。另外，如果一个分类属性（或特征）具有大量不同值，或者某些之出现不频繁，则对于某些数据挖掘任务，通过合并某些值减少类别的数目可能是有益的。</p><p>本质：将连续型数据分段</p><h4 id="无监督离散化">无监督离散化</h4><ul><li>等距离散化</li><li>等频离散化</li><li>基于聚类分析的离散化（先聚类，再合并（自底向上）或分裂（自顶向下），达到预定的簇个数）</li><li>基于正态3σ的离散化</li></ul><h4 id="有监督离散化">有监督离散化</h4><ul><li>基于信息增益的离散化（自顶向下）</li><li>基于卡方的离散化（自底向上，根据不同区间的类分布相似度进行聚合，相似度越高，卡方值越小）</li></ul><h4 id="小结">小结</h4><p><img src="../images/cl12.png" /></p><h2 id="缺失值处理">缺失值处理</h2><h3 id="缺失机制">缺失机制</h3><ul><li>完全随机缺失</li><li>随机缺失（与完全变量有关）</li><li>完全非随机缺失（与不完全变量自身有关）</li></ul><h3 id="处理方法">处理方法</h3><h4 id="删除法">删除法</h4><ul><li>删除样本</li><li>删除变量（缺得多，对研究目标影响不大）</li><li>降低权重（不影响数据结构）</li></ul><h4 id="基于填补的方法">基于填补的方法</h4><ul><li>单一填补（均值、k-means、热平台、冷平台）</li><li>随即填补（在均值填补的基础上加随机项）</li></ul><h4 id="基于模型的方法">基于模型的方法</h4><ul><li>建模预测</li></ul><h2 id="异常值检测">异常值检测</h2><h3 id="基于统计的方法">基于统计的方法</h3><p>上下α分位数之外的值</p><h3 id="基于距离的算法">基于距离的算法</h3><p>局部异常因子算法（LOF算法）</p>]]></content>
    
    
    <summary type="html">机器学习中的数据预处理简介</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据预处理" scheme="http://woody5962.github.io/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>集束搜索</title>
    <link href="http://woody5962.github.io/%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2/"/>
    <id>http://woody5962.github.io/%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2/</id>
    <published>2019-09-23T16:00:00.000Z</published>
    <updated>2021-07-12T04:04:32.994Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想">算法思想</h2><p>举例说明，在NLP领域中，经常会涉及机器翻译结果的搜索，比如在decoder中，我们可以使用贪心策略每次选择softmax值最高的，但是由于贪心解往往达不到最优，所以我们放松贪心约束，每次选择最优的k个，k便是集束宽度，这样对解的搜索叫做集束搜索。</p><h2 id="过程示例">过程示例</h2><p>假设字典为[a,b,c]，beam size选择2，则如下图有：</p><p><img src="../images/jishu1.png" /></p><ol type="1"><li><p>在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b</p></li><li><p>生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb</p></li></ol><p>不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。</p><p>显然集束搜索属于贪心算法，不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。</p><p>而贪心搜索由于每次考虑当下词的概率，而通常英文中有些常用结构，比如吴恩达网课中的例子：“is going”，出现概率较大，会导致模型最终生成的句子过于冗余，如“is visiting”和“is going to be visiting”。</p><p>贪心搜索可以认为beam size为1时的集束搜索特例。</p>]]></content>
    
    
    <summary type="html">集束搜索的思想和算法步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>模拟退火算法</title>
    <link href="http://woody5962.github.io/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-21T16:00:00.000Z</published>
    <updated>2021-07-01T10:56:58.588Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>这个方法来自金属热加工过程的启发。在金属热加工过程中，当金属的温度超过它的熔点（Melting Point）时，原子就会激烈地随机运动。与所有的其它的物理系统相类似，原子的这种运动趋向于寻找其能量的极小状态。在这个能量的变迁过程中，开始时，温度非常高， 使得原子具有很高的能量。随着温度不断降低，金属逐渐冷却，金属中的原子的能量就越来越小，最后达到所有可能的最低点。</p><h2 id="应用">应用</h2><p>利用模拟退火的时候，让算法从较大的跳跃开始，使到它有足够的“能量”逃离可能“路过”的局部最优解而不至于限制在其中，当它停在全局最优解附近的时候，逐渐的减小跳跃量，以便使其“落脚 ”到全局最优解上。</p><h2 id="具体操作">具体操作</h2><p>从一个问题的原始解开始，用一个变量代表温度，这一温度开始时非常高，而后逐步减低。在每一次迭代期间，算法会随机选中题解中的某个数字，使其发生细微变化，而后计算该解的代价。关键的地方在于计算出该解的代价后，如何决定是否接受该解。 如果新的成本更低，则新的题解就会变成当前题解，这与爬山法类似；如果新的成本更高，则新的题解与概率 P 被接受（一开始的时候，T很高，导致P很大，于是就有了足够的能量逃离局部最优）。这一概率会随着温度T的降低而降低。即算法开始时，可以接受表现较差的解，随着退火过程中温度的不断下降，算法越来越不可以接受较差的解，知道最后，它只会接受更优的解。 其中P = exp[-（newcost - oldcost）/ T ] 其中newcost是新解的成本，oldcost是当前成本，T为当前温度。算法以概率P接受新的解。可见，当温度很高或者新旧解差别很大时，概率P都会很大。</p>]]></content>
    
    
    <summary type="html">模拟退火算法的思想和算法步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="模拟退火算法" scheme="http://woody5962.github.io/tags/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>爬山法</title>
    <link href="http://woody5962.github.io/%E7%88%AC%E5%B1%B1%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E7%88%AC%E5%B1%B1%E6%B3%95/</id>
    <published>2019-09-21T16:00:00.000Z</published>
    <updated>2021-07-01T11:02:31.034Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想">算法思想</h2><p>爬山法假设当前解和周围的解是有变化规律的，如，当前解得下方有一个代价较小的解，则我们就认为，沿着这个方向走，解会越来越小。</p><p>该算法将搜索过程比作爬山过程，在没有任何有关山顶的其他信息的情况下，沿着高度增加的方向爬。如果相邻状态没有比当前值更高，则算法停止，认为当前值即为顶峰。</p><h2 id="算法步骤">算法步骤</h2><ol type="1"><li>设置初始状态n=s0为当前状态;</li><li>如果当前状态已达标，算法结束，搜索成功;</li><li>获取当前状态n的若干个临近状态m,计算这些h(m), nextn=min{h(m)}；</li><li>IF h(n) &gt; h(nextn) THEN n:=nextn; ELSE 取当前状态为最佳状态并退出;</li><li>GOTO 2步;</li></ol><p>实际操作中：</p><ul><li><p>循环改变解的每一个值产生一个临近解的列表</p></li><li><p>对所有的临近解计算代价并排序，选择代价符合要求的的解</p></li></ul><p>对于单峰来说，必定能找到最优点。</p>]]></content>
    
    
    <summary type="html">爬山法的思想和算法步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="爬山法" scheme="http://woody5962.github.io/tags/%E7%88%AC%E5%B1%B1%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>蚁群算法</title>
    <link href="http://woody5962.github.io/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-20T16:00:00.000Z</published>
    <updated>2021-07-12T04:05:59.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想">算法思想</h2><p>将蚁群算法应用于解决优化问题的基本思路为：用蚂蚁的行走路径表示待优化问题的可行解，整个蚂蚁群体的所有路径构成待优化问题的解空间。路径较短的蚂蚁释放的信息素量较多，随着时间的推进，较短的路径上累积的信息素浓度逐渐增高，选择该路径的蚂蚁个数也愈来愈多。最终，整个蚂蚁会在正反馈的作用下集中到最佳的路径上，此时对应的便是待优化问题的最优解。</p><h2 id="算法流程蚁群解决tsp">算法流程（蚁群解决TSP）</h2><p><img src="../images/yiqun1.jpg" /></p><h3 id="选择下一座城市的规则">选择下一座城市的规则</h3><p><img src="../images/yiqun2.png" /></p><ul><li><p>由选择公式可见，分母是所有可选路径的信息素的和，而某路径信息素越浓，选中它的概率越大。</p></li><li><p>同时，给出了beta参数作为启发信息的权重，在这里，启发信息正如其名，是一种启发，根据其初值可知，哪条路径最短，其被选中的概率就越高。在早期，就是这样的启发信息来促使算法少走弯路。</p></li></ul><h3 id="信息素更新规则">信息素更新规则</h3><p><img src="../images/yiqun3.png" /></p><p><img src="../images/yiqun4.png" /></p><ul><li><p>当Lk 计算完毕之后，得到目前最优解，根据最优解的路径更新信息素。</p></li><li><p>可见，信息素在更新之前，先削弱了旧信息素的影响，然后将最优路径的信息素添加进去，由于最优，所以L小，信息素大。</p></li></ul><h2 id="缺点">缺点</h2><p>实际实验中发现，当蚂蚁在一条路径上觅食很久时，放置一个近的食物基本没有效果，这可以理解为当一只蚂蚁找到一条路径时，在经过很长时间后大多数蚂蚁都选择了这条路径，这时，突然有一只蚂蚁找到了较近的食物，但因为时间过得太久，两条路径上浓度相差太大（浓度越大，被选择的概率就越大），整个系统基本已经停滞了，陷入了局部最优。</p>]]></content>
    
    
    <summary type="html">蚁群算法思想和步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="蚁群算法" scheme="http://woody5962.github.io/tags/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>局部搜索算法</title>
    <link href="http://woody5962.github.io/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2/"/>
    <id>http://woody5962.github.io/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2/</id>
    <published>2019-09-19T16:00:00.000Z</published>
    <updated>2021-07-01T10:50:40.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想">算法思想</h2><p>总是去找可能含有较优点的子集，最终找到的不一定是最优的点，但是具有较强的局部搜索能力。</p><h2 id="算法步骤">算法步骤</h2><ol type="1"><li>随机选择一个初始的可能解x0 ∈D，xb=x0,P=N(xb)。 D是问题的定义域， xb用于记录到目标位置的最优解，P为xb的邻域。</li><li>如果不满足结束条件，则：</li></ol><p>Begin； * 选择P的一个子集P'，xn为P’的最优解。P’可根据问题特点，选择适当大小的子集。可按概率选择； * 如果f(xn)&lt;f(xb)，则xb=xn，P=N(xb)，转2； * 否则P=P-P'，转2；</p><p>End（结束条件为循环次数或P为空等）；</p><ol start="3" type="1"><li>输出计算结果；</li><li>结束</li></ol>]]></content>
    
    
    <summary type="html">局部搜索的算法思想和步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>随机搜索算法</title>
    <link href="http://woody5962.github.io/%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2/"/>
    <id>http://woody5962.github.io/%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2/</id>
    <published>2019-09-19T16:00:00.000Z</published>
    <updated>2021-07-01T10:55:06.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于该算法">关于该算法</h2><p>随机搜索算法时最简单的优化搜索算法。</p><p>类似于调参过程中的随机搜索，当不知道代价函数在值域范围内的变化规律时，可以使用随机算法对解进行搜索。</p><h2 id="适用范围">适用范围</h2><p>只适用于代价函数在值域范围内没有任何变化规律的情况，即找不到任何使得代价下降的梯度和极小值点。</p><h2 id="求解">求解</h2><p>只需要在值域范围内生成足够多的可行解，然后分别计算每个可行解的代价，根据代价选择一个最小的可行解作为随机搜索的最优解即可。</p>]]></content>
    
    
    <summary type="html">随机搜索的简介</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>禁忌搜索算法</title>
    <link href="http://woody5962.github.io/%E7%A6%81%E5%BF%8C%E6%90%9C%E7%B4%A2/"/>
    <id>http://woody5962.github.io/%E7%A6%81%E5%BF%8C%E6%90%9C%E7%B4%A2/</id>
    <published>2019-09-18T16:00:00.000Z</published>
    <updated>2021-07-12T04:04:50.386Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想">算法思想</h2><p><img src="../images/jinji1.png" /> 通过保持一个禁忌表，以禁忌长度进行禁忌，遵守禁忌准则和藐视准则进行全局搜索。</p><h2 id="示例">示例</h2><p>首先，我们对置换问题定义一种邻域搜索结构，如互换操作（SWAP），即随机交换两个点的位置，则每个状态的邻域解有Cn2=n（n一1）/2个。称从一个状态转移到其邻域中的另一个状态为一次移动（move），显然每次移动将导致适配值（反比于目标函数值）的变化。其次，我们采用一个存储结构来区分移动的属性，即是否为禁忌“对象”在以下示例中：考虑7元素的置换问题，并用每一状态的相应21个邻域解中最优的5次移动（对应最佳的5个适配值）作为候选解；为一定程度上防止迂回搜索，每个被采纳的移动在禁忌表中将滞留3步（即禁忌长度），即将移动在以下连续3步搜索中将被视为禁忌对象；需要指出的是，由于当前的禁忌对象对应状态的适配值可能很好，因此在算法中设置判断，若禁忌对象对应的适配值优于“ best so far”状态，则无视其禁忌属性而仍采纳其为当前选择，也就是通常所说的藐视准则（或称特赦准则）。 注意，出于改善算法的优化时间性能的考虑，若领域结构决定了大量的领域解（尤其对大规模问题，如TSP的SWAP操作将产生Cn2个领域解），则可以仅尝试部分互换的结果，而候选解也仅取其中的少量最佳状态。</p><h2 id="算法步骤">算法步骤</h2><p>简单TS算法的基本思想是：给定一个当前解（初始解）和一种邻域，然后在当前解的邻域中确定若干候选解；若最佳候选解对应的目标值优于“best so far”状态，则忽视其禁忌特性，用其替代当前解和“best so far”状态，并将相应的对象加入禁忌表，同时修改禁忌表中各对象的任期；若不存在上述候选解，则选择在候选解中选择非禁忌的最佳状态为新的当前解，而无视它与当前解的优劣，同时将相应的对象加入禁忌表，并修改禁忌表中各对象的任期；如此重复上述迭代搜索过程，直至满足停止准则。</p><p>　　条理化些，则简单禁忌搜索的算法步骤可描述如下：</p><p>　　（1）给定算法参数，随机产生初始解x，置禁忌表为空。</p><p>　　（2）判断算法终止条件是否满足？若是，则结束算法并输出优化结果；否则，继续以下步骤。</p><p>　　（3）利用当前解工的邻域函数产生其所有（或若干）邻域解，并从中确定若干候选解。</p><p>　　（4）对候选解判断藐视准则是否满足？若成立，则用满足藐视准则的最佳状态y替代x成为新的当前解，即x=y，并用与y对应的禁忌对象替换最早进入禁忌表的禁忌对象，同时用y替换“best so far”状态，然后转步骤6；否则，继续以下步骤。</p><p>　　（5）判断候选解对应的各对象的禁忌属性，选择候选解集中非禁忌对象对应的最佳状态为新的当前解，同时用与之对应的禁忌对象替换最早进入禁忌表的禁忌对象元素。</p><p>　　（6）转步骤（2）。</p><p>注意，根据（5）可知，最牛逼的解根本就不考虑它是不是在禁忌列表中，如果这些候选解都很弟弟，那就找非禁忌里面最优的了。这样，既可以保证不遗漏全局最优，又有利于跳出局部最优。</p>]]></content>
    
    
    <summary type="html">禁忌搜索的算法思想和步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>粒子群优化算法</title>
    <link href="http://woody5962.github.io/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-18T16:00:00.000Z</published>
    <updated>2021-07-12T04:05:11.106Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>假设有一群鸟，在随机搜索食物，在搜索区域内只有一块儿食物，一开始时所有的鸟儿都不知道食物所在的方位，但它们能够知道自己离食物有多远，以及它们能够记住在自己飞过的路程当中距离食物最近的位置，同时它们也能够知道鸟群中所有鸟儿经过的路程当中，离食物最近的位置。那每一只鸟儿将如何去寻找食物呢？简单来说，每一只鸟儿在当前位置的基础上，如何做出决策，下一步向哪里飞呢？实际，每只鸟儿将综合自身的经验，以及群体的经验来在做出下一步飞向哪里的决策，即每只鸟儿将根据自己所经过的路程中离食物最近的位置以及鸟群中所有鸟儿经过的路程当中离食物最近的位置来做出决策，决定下一步自己向哪里飞。</p><p>在粒子群算法中，粒子的位置对应于原问题的解。粒子的适应值就是将粒子的位置（对应于原问题的解）带入到目标函数中所得到的目标函数值。粒子的速度决定粒子下一步向哪里飞以及飞多远。</p><h2 id="核心公式">核心公式</h2><p><img src="../images/pso1.png" /></p><h2 id="参数解释">参数解释</h2><p><img src="../images/pso2.png" /></p><h2 id="个人理解">个人理解</h2><p>粒子群算法整合了个体与整体的智慧，在“信息素”更新的时候，综合考虑了个体和整体在过去的发现。调整参数可以避免陷入局部最优，同时也可以决定对个体或者整体的依赖程度。个人认为，整体越大，对整体的依赖程度就要越大，这个依赖程度应该和粒子数量成正相关关系。</p>]]></content>
    
    
    <summary type="html">粒子群优化算法的背景、原理以及一些个人理解</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="粒子群算法" scheme="http://woody5962.github.io/tags/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>进化算法</title>
    <link href="http://woody5962.github.io/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://woody5962.github.io/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</id>
    <published>2019-09-17T16:00:00.000Z</published>
    <updated>2021-06-30T16:57:38.796Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>进化算法主要包括： - 遗传算法 - 进化程序设计 - 进化规划 - 进化策略</p><p>之后的文章记录遗传算法、进化规划、进化策略三种算法的理解。</p><h2 id="遗传算法">遗传算法</h2><h3 id="规则">规则</h3><p>适者生存，优胜劣汰！</p><h3 id="概念">概念</h3><h4 id="概念0编码与解码">概念0：编码与解码</h4><p>要将问题进行数学建模，将问题的解利用二进制数字串进行表示。 而进行适应度分析时，要将其解码成特征进行评价。</p><h4 id="概念1基因和染色体">概念1：基因和染色体</h4><p>在遗传算法中，我们首先需要将要解决的问题映射成一个数学问题，也就是所谓的“数学建模”，那么这个问题的一个可行解即被称为一条“染色体”。一个可行解一般由多个元素构成，那么这每一个元素就被称为染色体上的一个“基因”。</p><h4 id="概念2适应度函数">概念2：适应度函数</h4><p>给染色体打分（评价解的优良程度）</p><h4 id="概念3交叉">概念3：交叉</h4><p>遗传算法每一次迭代都会生成N条染色体，在遗传算法中，这每一次迭代就被称为一次“进化”。交叉算子种类很多，具体使用时找收藏夹，要注意有的交叉算子处理之后要进行冲突检查。</p><h4 id="概念4变异">概念4：变异</h4><p>当我们通过交叉生成了一条新的染色体后，需要在新染色体上随机选择若干个基因，然后随机修改基因的值，从而给现有的染色体引入了新的基因，突破了当前搜索的限制，更有利于算法寻找到全局最优解。交叉只能找到局部最优解，永远没有办法达到全局最优解。</p><h4 id="概念5复制">概念5：复制</h4><p>每次进化中，为了保留上一代优良的染色体，需要将上一代中适应度最高的几条染色体直接原封不动地复制给下一代。假设每次进化都需生成N条染色体，那么每次进化中，通过交叉方式需要生成N-M条染色体，剩余的M条染色体通过复制上一代适应度最高的M条染色体而来。</p><h4 id="概念6选择">概念6：选择</h4><p>如何从群体中找到染色体进行交叉，这涉及到一定的选择策略，常用的有轮盘赌选择策略、锦标赛选择策略、线性排序选择（Linear Ranking Selection）和指数排序选择（Exponential Ranking Selection）。后面两种选择策略避免了轮盘赌的一个缺点，那就是可以让适应度为0的个体也有机会产生后代（会确定一个最低概率）。</p><h3 id="流程">流程</h3><ol type="1"><li>在算法初始阶段，它会随机生成一组可行解，也就是第一代染色体。</li><li>采用适应度函数分别计算每一条染色体的适应程度，并根据适应程度计算每一条染色体在下一次进化中被选中的概率(这个上面已经介绍，这里不再赘述)。 （上面都是准备过程，下面正式进入“进化”过程)</li><li>通过“交叉”，生成N-M条染色体；</li><li>对交叉后生成的N-M条染色体进行“变异”操作；</li><li>使用“复制”的方式生成M条染色体；</li><li>N条染色体生成完毕,紧接着分别计算N条染色体的适应度和下次被选中的概率。 (这就是一次进化的过程，紧接着进行新一轮的进化)</li></ol><h3 id="超参数">超参数</h3><p>进化次数允许范围：算法到达一定效果就停止，保证效率。</p><h2 id="进化策略">进化策略</h2><h3 id="流程-1">流程</h3><ol type="1"><li>编码：对要求解的问题以数字串的方式进行编码（由目标参数和策略参数组成），计算合适度值</li><li>判断是否满足终止条件：如满足则输出结果；否则执行下述步骤</li><li>选择n个父代参与繁殖</li><li>按给定的方式执行交叉操作（可选）</li><li>基于高斯分布的扰动执行变异操作</li><li>产生m个子代，并计算合适度值（m&gt;n）</li><li>返回步骤2</li></ol><h2 id="进化规划">进化规划</h2><ol type="1"><li>编码：对要求解的问题以数字串的方式进行编码（由目标参数和策略参数组成），计算合适度值</li><li>判断是否满足终止条件：如满足则输出结果；否则执行下述步骤</li><li>选择n个父代参与繁殖</li><li>基于高斯分布的扰动执行变异操作</li><li>产生n个子代，并计算合适度值</li><li>返回步骤2</li></ol><h2 id="进化策略进化规划与遗传算法">进化策略、进化规划与遗传算法</h2><ol type="1"><li><p>编码方面：不像传统遗传算法那样需要对要求解的问题进行0-1编码和解码，而是直接对求解的问题进行编码，即直接将优化问题的解表示为数字串的形式，不需要特定的编码和译码，可直接进行实数编码。</p></li><li><p>均采用同样的变异操作方式，即变异时，对父代中的个体加上一个服从均值为0，标准差为 σ 的高斯分布随机变量。标准差是变化的，编码时属于染色体串中的一部分。</p></li></ol><h2 id="进化策略与进化规划的差别">进化策略与进化规划的差别</h2><ol type="1"><li>进化策略中的交叉算子是可选的；如需要进行交叉运算时，采用类似遗传算法的处理方法，如离散交叉或中值交叉方式。进化规划没有交叉算子。</li><li>父代选择方面：进化策略采用概率选择的方法形成父代（如基于随机分布的方式抽取父代个体），父代每一个个体都能以同样的概率被选中。 进化规划采用确定性方式，即当前种群中每个父代都要经过变异来产生子代。</li><li>变异表达式上的差异。</li><li>生存选择方面，即新的子代生成后，如何和父代共同形成新的父代的方式。</li></ol>]]></content>
    
    
    <summary type="html">进化算法主要涵盖的算法，包括其中各种算法的区别和联系</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="进化算法" scheme="http://woody5962.github.io/tags/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>启发式算法概述</title>
    <link href="http://woody5962.github.io/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://woody5962.github.io/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</id>
    <published>2019-09-09T16:00:00.000Z</published>
    <updated>2021-07-01T13:01:09.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义">定义</h2><p>一个基于直观或经验构造的算法，在可接受的花费（计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。</p><p>现阶段，启发式算法以仿自然体算法为主，主要有蚁群算法、模拟退火法、神经网络等。</p><p>个人理解，所谓启发就是，没有精确的数学公式得到直接解的情况下，告诉机器该如何去发现指导信息，进而根据指导信息去寻找解。</p><h2 id="元启发式算法">元启发式算法</h2><ul><li><a href="../模拟退火算法/">模拟退火算法</a></li><li><a href="../进化算法概述/">遗传算法</a></li><li>列表搜索算法</li><li><a href="../进化算法概述/">进化规划</a></li><li><a href="../进化算法概述/">进化策略</a></li><li><a href="../蚁群算法/">蚁群算法</a></li><li>人工神经网络</li><li><a href="../禁忌搜索/">禁忌搜索</a></li><li><a href="../粒子群算法/">粒子群优化</a></li></ul><h2 id="超启发式算法">超启发式算法</h2><ul><li>基于随机选择的超启发式算法</li><li>基于贪心策略的超启发式算法</li><li>基于元启发式算法的超启发式算法</li><li>基于学习的超启发式算法</li></ul>]]></content>
    
    
    <summary type="html">启发式算法的定义和分类，以及一些个人的理解</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
