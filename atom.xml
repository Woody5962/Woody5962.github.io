<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Woody Tells</title>
  
  <subtitle>Never Settle</subtitle>
  <link href="http://woody5962.github.io/atom.xml" rel="self"/>
  
  <link href="http://woody5962.github.io/"/>
  <updated>2022-10-16T12:23:30.328Z</updated>
  <id>http://woody5962.github.io/</id>
  
  <author>
    <name>Woody</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python多进程和多线程</title>
    <link href="http://woody5962.github.io/Python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    <id>http://woody5962.github.io/Python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B/</id>
    <published>2022-10-16T09:14:33.000Z</published>
    <updated>2022-10-16T12:23:30.328Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><ol><li>CPU的分时复用</li></ol><p>目前我们普遍使用多核CPU，可以多核同时执行多个任务，但是即使是单核时代，计算机也是可以同时执行多个任务的，其原理是操作系统轮流让各个任务交替执行，即分时复用。由于计算机运算速度较快，所以即使是每个任务只能间歇运算，也基本可以满足使用要求。</p><ol start="2"><li>进程</li></ol><p>进程是对运行程序的封装，是系统进行资源调度和分配的的基本单位，用以实现操作系统的并发。进程在执行过程中拥有独立的内存单元，且不同进程间不会相互影响，适应于多核、多机分布运行。</p><ul><li>系统开销：由于在创建或撤消进程时，系统都要为之分配或回收资源，如内存空间、I／o设备等。因此，操作系统所付出的开销将显著地大于在创建或撤消线程时的开销。类似地，在进行进程切换时，涉及到整个当前进程CPU环境的保存以及新被调度运行的进程的CPU环境的设置。</li><li>通信：进程间通信主要包括管道、系统IPC（包括消息队列、信号量、信号、共享内存等）、以及套接字socket。</li></ul><ol start="3"><li>线程</li></ol><p>线程是进程的子任务，是CPU调度和分派的基本单位，用于保证程序的实时性，实现进程内部的并发。每个线程都独自占用一个虚拟处理器：独自的寄存器组，指令计数器和处理器状态。每个线程完成不同的任务，但是共享进程的资源。</p><ul><li>系统开销：线程切换只须保存和设置少量寄存器的内容，并不涉及存储器管理方面的操作。可见，<strong>进程切换的开销也远大于线程切换的开销</strong>。</li><li>通信：由于同一进程中的多个线程具有相同的地址空间，致使它们之间的同步和通信的实现，也变得比较容易。进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。<strong>在有的系统中，线程的切换、同步和通信都无须操作系统内核的干预</strong>。</li></ul><h2 id="Python如何同时执行多任务"><a href="#Python如何同时执行多任务" class="headerlink" title="Python如何同时执行多任务"></a>Python如何同时执行多任务</h2><p>有三种解决方案：</p><ol><li><p>启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。</p></li><li><p>启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。</p></li><li><p>启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。</p></li></ol><p>多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。</p><p>Python既支持多进程，又支持多线程，下面讲解一下如何实现Python多任务编程。</p><h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><p>Unix/Linux操作系统提供了一个<code>fork()</code>系统调用，调用之后操作系统自动从父进程复制出来一份子进程。<br>由于Windows没有<code>fork</code>调用，所以Python提供了一套跨平台的多进程支持：<code>multiprocessing</code>，该模块提供了一个<code>Process</code>类来代表一个进程对象，在父进程执行过程中，只需要调用<code>Process(target, args)</code>就可以生成一个子进程，子进程可以用<code>start()</code>方法启动，并且提供了<code>join()</code>方法用来使父进程等待子进程结束后再继续往下运行，通常用于进程间的同步。</p><p>我们最常用的应该是<code>multiprocessing</code>提供的<code>Pool</code>进程池，直接生成多个子进程来执行同一个任务，比如大批量文件处理等。一般来说，设置进程数目为<code>os.cpu_count()</code>比较合适，可以使每个核都运行一个进程。</p><p>为实现进程间的通信，<code>multiprocessing</code>包装了一些底层的机制，提供了<code>Queue</code>、<code>Pipes</code>等多种方式来交换数据。以<code>Queue</code>为例，可以在父进程中创建两个子进程，一个往<code>Queue</code>里写数据，一个从<code>Queue</code>里读数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"><span class="keyword">import</span> os, time, random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写数据进程执行的代码:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span>(<span class="params">q</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process to write: %s&#x27;</span> % os.getpid())</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Put %s to queue...&#x27;</span> % value)</span><br><span class="line">        q.put(value)</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读数据进程执行的代码:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span>(<span class="params">q</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process to read: %s&#x27;</span> % os.getpid())</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        value = q.get(<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Get %s from queue.&#x27;</span> % value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 父进程创建Queue，并传给各个子进程：</span></span><br><span class="line">    q = Queue()</span><br><span class="line">    pw = Process(target=write, args=(q,))</span><br><span class="line">    pr = Process(target=read, args=(q,))</span><br><span class="line">    <span class="comment"># 启动子进程pw，写入:</span></span><br><span class="line">    pw.start()</span><br><span class="line">    <span class="comment"># 启动子进程pr，读取:</span></span><br><span class="line">    pr.start()</span><br><span class="line">    <span class="comment"># 等待pw结束:</span></span><br><span class="line">    pw.join()</span><br><span class="line">    <span class="comment"># pr进程里是死循环，无法等待其结束，只能强行终止:</span></span><br><span class="line">    pr.terminate()</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">Process to write: <span class="number">11560</span></span><br><span class="line">Put A to queue...</span><br><span class="line">Process to read: <span class="number">10976</span></span><br><span class="line">Get A <span class="keyword">from</span> queue.</span><br><span class="line">Put B to queue...</span><br><span class="line">Get B <span class="keyword">from</span> queue.</span><br><span class="line">Put C to queue...</span><br><span class="line">Get C <span class="keyword">from</span> queue.</span><br></pre></td></tr></table></figure><blockquote><p>在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。</p></blockquote><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。</p><p>启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading, multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span>():</span></span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x = x ^ <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(multiprocessing.cpu_count()):</span><br><span class="line">    t = threading.Thread(target=loop)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure><h3 id="支持多线程，但不完全支持"><a href="#支持多线程，但不完全支持" class="headerlink" title="支持多线程，但不完全支持"></a>支持多线程，但不完全支持</h3><p>python使用多线程只能用到CPU的一个核心，因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，  任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。<strong>所以，在Python中，可以使用多线程，但不要指望能有效利用多核</strong>。</p><p>一般来说，python多线程对IO密集型任务比较友好，因为IO等待期间可以自动切换到其他线程，不浪费CPU资源，从而提升程序执行效率。但是对于CPU密集型任务则很不友好，甚至效率会低于单线程，因为python会在切换线程上浪费很多时间。</p><p>下面的程序很直观的展示出单线程、多线程和多进程的运行效率区别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor, ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testadd</span>():</span></span><br><span class="line">    s = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(<span class="number">1e7</span>)):</span><br><span class="line">        s += i</span><br><span class="line">        s /= <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">time1 = time.perf_counter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    testadd()</span><br><span class="line">time2 = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;time2 - time1:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pool = ThreadPoolExecutor(max_workers=<span class="number">8</span>)</span><br><span class="line">tasks = [pool.submit(testadd) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tasks:</span><br><span class="line">    t.result()</span><br><span class="line">time3 = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;time3 - time2:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pool = ProcessPoolExecutor(max_workers=<span class="number">8</span>)</span><br><span class="line">tasks = [pool.submit(testadd) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tasks:</span><br><span class="line">    t.result()</span><br><span class="line">time4 = time.perf_counter()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;time4 - time3:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line"><span class="number">6.33</span></span><br><span class="line"><span class="number">17.38</span></span><br><span class="line"><span class="number">1.06</span></span><br></pre></td></tr></table></figure><p>可以看出，当单线程耗时6秒时，多线程耗时17秒！几乎3倍的时间！而多进程时间仅为1秒！</p><h3 id="Python并行多线程的进展"><a href="#Python并行多线程的进展" class="headerlink" title="Python并行多线程的进展"></a>Python并行多线程的进展</h3><p>Python的核心开发者会不定期对外公示PEP（Python Enhancement Proposals），作为review之后得到公认的开发提案，其官网为：<a href="https://www.python.org/dev/peps/%E3%80%82CPython%E7%9A%84%E6%A0%B8%E5%BF%83%E5%BC%80%E5%8F%91%E5%A4%A7%E4%BD%ACEric">https://www.python.org/dev/peps/。CPython的核心开发大佬Eric</a> Snow于今年3月8日提出了PEP-684，简单明了的指出了要在Python3.12版本为每个子解释器维护一个GIL：</p><p><img src="../images/multi1.png"></p><p>由于GIL控制着全局状态的访问，导致同一进程下的多个线程只能分时复用计算资源。所以PEP-684提出一个解决方案，保证所有的全局状态是线程安全的，并未每个子解释器维护一个GIL，从而保证了多个线程可以安全且并行地使用计算资源。</p><p>在刚刚过去的9月份，Eric Snow在<a href="https://discuss.python.org/t/pep-684-a-per-interpreter-gil/19583">https://discuss.python.org/t/pep-684-a-per-interpreter-gil/19583</a> 向大家公示了PEP-684的一些最新讨论结果，虽然该提案目前仍处在draft阶段，但毕竟是有了眉目。</p><p>不妨一起期待Python3.12的发布！</p>]]></content>
    
    
    <summary type="html">Python多进程和多线程现状与最新进展</summary>
    
    
    
    <category term="Python" scheme="http://woody5962.github.io/categories/Python/"/>
    
    
    <category term="Python" scheme="http://woody5962.github.io/tags/Python/"/>
    
    <category term="多进程/多线程" scheme="http://woody5962.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>稳定学习</title>
    <link href="http://woody5962.github.io/%E7%A8%B3%E5%AE%9A%E5%AD%A6%E4%B9%A0/"/>
    <id>http://woody5962.github.io/%E7%A8%B3%E5%AE%9A%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-09-09T16:00:00.000Z</published>
    <updated>2021-09-10T07:49:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Stable-Learning简介"><a href="#Stable-Learning简介" class="headerlink" title="Stable Learning简介"></a>Stable Learning简介</h2><h3 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h3><ol><li>识别一张图片中是否有狗。在很多预测问题中，我们拿到的数据集往往都是有偏的，比如我们拿到的数据中有80%的图片中狗都在草地上，这样就导致在训练集中草地这一特征会和图片中是否有狗这个label十分相关。基于这样的有偏数据集学习一个预测模型，无论是LR，还是Deep Model，都很有可能会将草地这一特征学习成很重要的预测特征。<br><img src="../images/%E6%9C%AA%E5%91%BD%E5%90%8D%E5%9B%BE%E7%89%87.png"></li><li>如果要帮助医院预测一个癌症患者的生存率，我们很难拿到所有医院的数据，假设我们现在拿到了某一个城市某一个医院的数据，如果我们利用这个数据集做建模，我们可能会发现在这个医院中病人的收入越高，病人的幸存率也会越高，这是有道理的，收入高的病人得到的治疗和能支付起的药物可能更好。基于这样的模型做预测时，如果未来的要预测的病人同样是来自该医院的患者，我们可能会得到很准确的预测结果。但是如果未来要预测的数据集来自军队医院、学校医院等对患者收入没有要求的医院，此时的预测效果很可能不好。<br><img src="../images/s2.png"></li></ol><h3 id="稳定学习的动机"><a href="#稳定学习的动机" class="headerlink" title="稳定学习的动机"></a>稳定学习的动机</h3><p>目前深度学习在很多研究领域特别是计算机视觉领域（如图像识别、物体检测等技术领域）取得了前所未有的进展，而深度模型性能依赖于模型对训练数据的拟合。当训练数据（应用前可获取的数据）与测试数据（实际应用中遇到的实例）分布不同时，传统深度模型对训练数据的充分拟合会造成其在测试数据上的预测失败，进而导致模型应用于不同环境时的可信度降低。稳定学习就是为了提高模型在任意未知应用环境中的准确率和稳定性。<br><img src="../images/s3.png"><br>上图（来自崔鹏老师的slide）给出了常见的独立同分布模型、迁移学习模型和稳定学习模型的异同。</p><p>独立同分布(i.i.d)模型的训练和测试都在相同分布的数据下完成，测试目标是提升模型在测试集上的准确度，对测试集环境有较高的要求；<br>迁移学习同样期望提升模型在测试集上的准确度，但是允许测试集的样本分布与训练集不同。独立同分布学习和迁移学习都要求测试集样本分布已知；<br>而稳定学习则希望在保证模型平均准确度的前提下，降低模型性能在各种不同样本分布下的准确率方差。理论上稳定学习可以在不同分布的测试集下都有较好的性能表现。</p><h2 id="OOD泛化中的深度稳定学习"><a href="#OOD泛化中的深度稳定学习" class="headerlink" title="OOD泛化中的深度稳定学习"></a>OOD泛化中的深度稳定学习</h2><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p><img src="../images/s13.png"></p><p>大致思路就是通过上面的LSWD分支学习采样权重，在损失计算阶段进行加权，模拟重采样，实现特征分布之间的独立性，尽量去除所有特征之间的相关性，从而解决DG问题中不相关变量对模型鲁棒性的影响。</p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>深度学习在很多领域的成功得益于iid假设，但是对于ood则会有很大的问题。所以消除分布偏移至关重要。</p><p>传统方法消除分布偏移往往基于以下假设：<br>  a. 显式domain label（大量的额外标注，且人类认知有限，无法对域）<br>  b. 潜在领域的平衡采样（假设本身就很脆弱）</p><p>本文抛弃上述假设，通过去除特征之间的依赖，进而去除不相关特征与标签的错误相关性，使得模型的关注点落在真正相关的特征上。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>首先想要明确，ood泛化性的研究就是为了是模型在存在分布偏移的时候依然可以有不错的效果。</p><p>现在模型在OOD性能下降的根本原因在于不相关特征与标签的错误相关性，而不相关特征能与标签建立相关性是由于不同数据分布下，特征之间本身就存在着微妙的相关性，比如狗一般出现在陆地上，鱼一般出现在水里，鸟一般出现在树上等等，这就会导致陆地、水、树与最终的标签产生错误的相关性。</p><p>OOD现有的解决方案可以分为两类：</p><ol><li>Domain generation：显式domain label</li><li>Learn latent domains from data：潜在领域的平衡采样</li></ol><p>目前有一个更加有效的思路：直接剔除相关特征和不相关特征之间的相关性。但特征是否相关是一件无法得知的事情，所以不妨去除所有特征之间的关联。</p><p>去相关性的现有解决方案主要集中在线性框架，并取得了不错的成效，大致思路是通过使混杂变量平衡（Confounder Balancing）的方法来使得神经网络模型能够推测因果关系 。具体而言，如果要推断变量A对变量B的因果关系（存在干扰变量C），以变量A是离散的二元变量（取值为0或1）为例，根据A的值将总体样本分为两组（A=0或A=1），并给每个样本赋予不同的权重，使得在A=0和A=1时干扰变量C的分布相同（即D(C|A=0) = D(C|A=1)，其中D代表变量分布），此时判断D(B|A=0) 和D(B|A=1)是否相同可以得出A是否与B有因果关系。</p><p>但上述方法扩展到深度模型会面临两个主要挑战：</p><ol><li>特征之间复杂的非线性依赖关系比线性依赖关系更难度量和消除；</li><li>全局样本加权策略对深度模型的存储量和计算量要求都比较大，这在实际中是不可行的。</li></ol><p>针对这两个挑战，本文有如下解决方案：</p><ol><li>针对上面说到的第一个挑战，本文提出了一种新的基于随机傅立叶特征的非线性特征去相关方法。</li><li>对于上面说到的第二个挑战，本文提出了一种高效的优化机制，通过迭代保存和重新加载模型的特征和权重来感知和去除全局相关性。且该方法对这两个模块进行了联合优化。</li></ol><h3 id="针对DG提出的采样方案"><a href="#针对DG提出的采样方案" class="headerlink" title="针对DG提出的采样方案"></a>针对DG提出的采样方案</h3><h4 id="独立性检验统计量"><a href="#独立性检验统计量" class="headerlink" title="独立性检验统计量"></a>独立性检验统计量</h4><p>如果非线性相关性处理比较麻烦，不妨使用核方法将其映射到高维空间（参考kernel SVM之类的思想），将非线性相关性转化为高维空间的线性相关性，从而降低难度。</p><p>衡量随机变量线性相关性的常用统计量为pearson相关系数，如果要去除随机变量之间的线性相关性，就可以考虑使pearson相关系数降为0，也就是使得二者协方差降为0</p><p>基于上述分析，使用核方法将随机变量映射到无穷维空间，构造再生核希尔伯特空间（RKHS），在该空间中，我们可以定义协方差算子Σ，根据Hilbert-Schmidt独立性准则（HSIC），我们只需要使得协方差算子的hilbert-schmidt范数为0即可保证两个元素的独立性。<br><img src="../images/s4.png"></p><p>由于在欧式空间，F范数与hilbert-schmidt范数是一致的。所以，在样本数目为n的数据集上，我们可以先将协方差算子建立为协方差矩阵的形式：<br><img src="../images/s5.png"></p><p>但由于RKHS空间为无穷维，上式无法直接进行计算。刚好随机傅立叶特征（RFF）对任意核函数都有不错的近似作用，所以我们不妨使用随机傅立叶特征将映射空间降到d维，从而使上式中的F范数进行计算：<br><img src="../images/s6.png"><br><img src="../images/s7.png"></p><p>综上所述，独立性检验统计量的计算就可以分为两步，首先通过RFF进行特征映射，然后进行协方差矩阵的F范数作为独立性检验统计量。</p><h4 id="学习样本权重用于去相关性"><a href="#学习样本权重用于去相关性" class="headerlink" title="学习样本权重用于去相关性"></a>学习样本权重用于去相关性</h4><p>有了之前的分析，这里就是很自然的想法了。我们通过学习采样权重（模拟采样，在loss上做手脚）来改变不同特征的分布，控制不同特征之间的独立性。</p><p>首先，确定采样权重来平衡混杂变量：<br><img src="../images/s8.png"><br><img src="../images/s9.png"></p><p>然后，将表征函数f和分类函数g加入损失计算，得到StableNet的目标函数：<br><img src="../images/s10.png"></p><h4 id="全局样本权重的学习"><a href="#全局样本权重的学习" class="headerlink" title="全局样本权重的学习"></a>全局样本权重的学习</h4><p>上述公式要求在训练过程中为每个训练样本都学习一个特定的权重，但在实践中，尤其对于深度学习任务，要想利用全部样本全局地学习样本权重需要巨大的计算和存储开销。<br>此外，使用SGD对网络进行优化时，每轮迭代中仅有部分样本对模型可见，因此无法获取全部样本的特征向量。<br>本文提出了一种存储、重加载样本特征与样本权重的方法，在每个训练迭代的结束融合并保存当前的样本特征与权重，在下一个训练迭代开始时重加载，作为训练数据的全局先验知识优化新一轮的样本权重，如下图所示：<br><img src="../images/s11.png"></p><p>在每一轮训练结束后，将本次的Z和w融合到全局信息中，如下图所示：<br><img src="../images/s12.png"></p><p>k表示一个倍数，也就是presaved的特征数目是原始特征的k倍。可以看成一个trick，后面作者做了ablation study证明了k的效果。</p><p>得到权重之后，通过在loss中加权模拟重采样来实现特征之间的去相关性。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>在常规的域泛化（DG）任务中，训练集的不同源域容量相近且异质性清晰，然而在实际应用中，绝大部分数据集都是若干潜在源域的组合，当源域异质性不清晰或未被显式标注时，我们很难假定来自于各源域的数据数量大致相同。为了更加全面地验证StableNet的泛化性能，本文提出三种新的域泛化任务来仿真更加普适且挑战性更强的分布迁移泛化场景，同时也比较了传统DG设定下的实验结果。</p><h4 id="不均衡的域泛化"><a href="#不均衡的域泛化" class="headerlink" title="不均衡的域泛化"></a>不均衡的域泛化</h4><p>对于源域不明确的域泛化问题，假定源域容量相近过于理想化，一个更普适的假设为来自不同源域的数据量可能不同且可能差异巨大。在这种情况下，模型对于未知目标域的泛化能力更满足实际应用的需求。例如在识别狗的例子中，我们很难假定背景为草地、沙滩或水里的图片数量相同，实际情况下狗较多地出现在草地上而较少出现在水里。这就要求模型的预测不能被经常与狗一起出现的背景草地误导，所以本任务的普适性和难度显著高于均衡的域泛化。<br><img src="../images/s14.png"></p><h4 id="部分类别缺失的域泛化"><a href="#部分类别缺失的域泛化" class="headerlink" title="部分类别缺失的域泛化"></a>部分类别缺失的域泛化</h4><p>考虑一种挑战性更大且在现实场景中经常存在的情况，某些源域中有部分类别的数据缺失，而在测试集中模型需要识别所有类别。例如，鸟经常出现在树上而几乎不会出现在水里，鱼经常出现鱼缸里而几乎不会出现在树上，所以并不是所有源域都一定包含全部类别。这种场景要求更高的模型泛化能力，由于每个源域中仅有部分类别，所以域相关的特征与标签间的虚假关联更强且更易误导分类器。<br><img src="../images/s15.png"></p><h4 id="存在对抗的域泛化"><a href="#存在对抗的域泛化" class="headerlink" title="存在对抗的域泛化"></a>存在对抗的域泛化</h4><p>一种难度更大的场景是任一给定类别的主导源域与主导目标域不同。例如，训练数据中的狗大多在草地上而猫大多在室内，而测试数据中的狗大多在室内而猫大多在草地上，这就导致如果模型不能区分本质特征与域相关特征，就会被域信息所误导而做出错误预测。下表为在MNIST-M数据集上的实验结果，StableNet仍显著优于其他方法，且可见随主导域比例升高，ResNet的表现显著下降，StableNet的优势也越发明显。<br><img src="../images/s16.png"></p><h4 id="传统DG设定"><a href="#传统DG设定" class="headerlink" title="传统DG设定"></a>传统DG设定</h4><p>数据集中的所有类别的样本都有相同的域集合。<br><img src="../images/s17.png"></p><h4 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h4><p>该消融实验主要是验证RFF采样数目和presaved特征数目对结果的影响，同时也比较了去除线形相关性和非线性相关性对结果的影响。<br><img src="../images/s18.png"></p><h3 id="代码可用性"><a href="#代码可用性" class="headerlink" title="代码可用性"></a>代码可用性</h3><p>源码： <a href="https://github.com/xxgege/StableNet">https://github.com/xxgege/StableNet</a></p><ul><li>源代码与论文保持一致，基于pytorch开发，可直接适配CV数据</li><li>代码支持分布式多卡训练以及resume训练等操作</li><li>训练过程包括两个学习过程<ul><li>采样权重学习</li><li>分类网络参数学习</li></ul></li><li>模型backbone为resnet，prediction分支的输出产生loss，与学到的weight进行加权（模拟重采样）进行回传</li><li>代码可读性较高，改造比较容易：<ul><li>修改backbone：非CV相关任务可以直接改成fc或者transformer encoder，用于特征初步提取与映射。</li><li>权重学习分支无需修改</li><li>主任务分支只需要对应特定任务即可</li></ul></li><li>训练脚本的dataloader需要重写</li></ul>]]></content>
    
    
    <summary type="html">稳定学习在解决OOD泛化问题上的应用</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="因果推断" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="因果推断" scheme="http://woody5962.github.io/tags/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
  </entry>
  
  <entry>
    <title>离散化与稠密化</title>
    <link href="http://woody5962.github.io/%E7%A6%BB%E6%95%A3%E5%8C%96%E4%B8%8E%E7%A8%A0%E5%AF%86%E5%8C%96/"/>
    <id>http://woody5962.github.io/%E7%A6%BB%E6%95%A3%E5%8C%96%E4%B8%8E%E7%A8%A0%E5%AF%86%E5%8C%96/</id>
    <published>2021-01-14T16:00:00.000Z</published>
    <updated>2022-10-16T08:22:38.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>离散化与稠密化是机器学习领域的两个重要概念，虽然不是互逆的概念，但是二者却有共通之处。</p><p>离散化的本质是将连续型数据分段，而稠密化的本质是降低表征维度。</p><p>前者是在单一维度特征内部进行”降维“，而后者是在不同特征之间进行实打实的降维。</p><p>下面对这两个概念分别进行介绍。</p><h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p>有些数据挖掘算法，特别是某些分类算法（如朴素贝叶斯），要求数据是分类属性形式（类别型属性），这样常常需要将连续属性变换成分类属性。</p><p>另外，如果一个分类属性（或特征）具有大量不同值，或者某些值出现不频繁，则对于某些数据挖掘任务，通过合并某些值减少类别的数目可能是有益的，具体优点分析如下：</p><ul><li>避免one-hot场景下频繁扩充维度，并且容易造成维度不匹配的问题</li><li>离散化后的特征对异常数据有很强的鲁棒性，比如我们可以把异常数据归并到一类里面，给一个统一的值，避免对结果造成过大的干扰。</li><li>离散化之后可以有效的进行特征交叉，提升模型表达能力</li><li>减轻过拟合风险，提升模型稳定性。比如31岁和30岁并没有太大的区别，不能因为长了一岁就对结果有了明显的影响；并且对于逻辑回归这种模型来说，离散化会减少参数量，显式的简化了模型，减轻了过拟合。</li></ul><h3 id="无监督离散化"><a href="#无监督离散化" class="headerlink" title="无监督离散化"></a>无监督离散化</h3><ul><li>等距离散化</li><li>等频离散化</li><li>基于聚类分析的离散化（先聚类，再合并（自底向上）或分裂（自顶向下），达到预定的簇个数）</li><li>基于正态3σ的离散化</li></ul><h3 id="有监督离散化"><a href="#有监督离散化" class="headerlink" title="有监督离散化"></a>有监督离散化</h3><ul><li>基于信息增益的离散化（自顶向下）</li><li>基于卡方的离散化（自底向上，根据不同区间的类分布相似度进行聚合，相似度越高，卡方值越小）</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><img src="../images/li1.png"></p><h2 id="稠密化"><a href="#稠密化" class="headerlink" title="稠密化"></a>稠密化</h2><p>稠密化在深度学习中有一个更常见的称呼——Embedding。一般来说，我们稠密化的对象是稀疏向量，最常见的是one-hot向量。</p><p>举一个简单的例子，在NLP中，我们会遇到多种one-hot向量，比如单词的原始表征、tf-idf表征等等，动辄就几十万维。对于这种表征，如果不进行稠密化，对空间复杂度和时间复杂度都是一个很大的考验。</p><p>将其稠密化为embedding之后，起码有如下优势：</p><ul><li>可以在不丢失信息的情况下实现降维，降低空间复杂度和时间复杂度。（众所周知，3维就可以表征8个状态，但是one-hot需要8维）</li><li>可以在嵌入空间赋予向量物理意义，使得向量之间可以有效的比较相似性，比如常见的在嵌入空间做余弦相似度。</li><li>避免one-hot的扩充和维度不对应问题</li></ul><p>具体做embedding的方法，目前已经多不胜数。最为经典的还是word2vec系列，并且近几年深度学习推荐系统的发展更是推进了embedding方法的扩展，graph embedding已经在推荐系统领域得到了广泛应用，在graph中可以灵活的添加side-information，甚至直接结合knowledge-graph，使得embedding的语义信息越来越丰富。</p>]]></content>
    
    
    <summary type="html">离散化与稠密化的动机以及优劣分析</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据预处理" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据预处理" scheme="http://woody5962.github.io/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
    <category term="特征表示" scheme="http://woody5962.github.io/tags/%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA/"/>
    
  </entry>
  
  <entry>
    <title>经典分类算法简介</title>
    <link href="http://woody5962.github.io/%E7%BB%8F%E5%85%B8%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/"/>
    <id>http://woody5962.github.io/%E7%BB%8F%E5%85%B8%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/</id>
    <published>2020-10-09T16:00:00.000Z</published>
    <updated>2021-09-10T07:46:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典分类算法"><a href="#经典分类算法" class="headerlink" title="经典分类算法"></a>经典分类算法</h2><p><img src="../images/fl1.png"></p><h3 id="贝叶斯方法体系"><a href="#贝叶斯方法体系" class="headerlink" title="贝叶斯方法体系"></a>贝叶斯方法体系</h3><p>不同的贝叶斯方法的区别在于计算后验概率时的分子，也就是联合分布概率。贝叶斯网络的联合分布可以根据网络图很直观地得到结果，具体可以学习一下概率图模型中的贝叶斯网络（与马尔可夫网络的区别就在于后者具有最大团属性，对应于马尔可夫性）。</p><ol><li>朴素贝叶斯</li></ol><ul><li>假设所有特征是独立的，无相互作用，可直接通过先验概率去计算后验概率。</li></ul><ol start="2"><li>半朴素贝叶斯</li></ol><ul><li>适当考虑一部分属性间的相互依赖信息。</li><li>典型的是独依赖估计 </li></ul><ol start="3"><li>贝叶斯网</li></ol><ul><li>也叫信念网（belief network）</li><li>借助有向无环图来刻画属性之间的依赖关系，使用条件概率表来描述属性的联合概率分布。</li><li>贝叶斯网络的大致原理是构造各维特征之间的依赖网络（通过某种基于信息论准则的学习过程构建），然后根据学习到的贝叶斯网络B（G，P）通过Gibbs采样来模拟类别概率。</li></ul><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>最简单的一种分类思路，利用样本点周围样本的类别来做分类，唯一需要调整的参数就是所用邻居样本点的数目k。</p><p><img src="../images/fl2.png"></p><p>另外，根据不同的数据特点确定距离度量也至关重要。</p><h3 id="常见的距离度量"><a href="#常见的距离度量" class="headerlink" title="常见的距离度量"></a>常见的距离度量</h3><ul><li>Minkowski-distance</li><li>Cosine similarity</li><li>Jaccard similarity：类别向量常用的一种距离度量，具体可见<a href="../%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/">数据预处理</a></li><li>KL-divergence</li></ul><h3 id="k的选取"><a href="#k的选取" class="headerlink" title="k的选取"></a>k的选取</h3><ul><li>网格搜索</li><li>随机搜索</li><li>高斯优化</li></ul><p>配合n-fold交叉验证选取合适的k。</p><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>分类使用的决策树根据不同的启发函数进行分枝，启发函数使用的一般都是信息论中的度量，比如信息增益、GINI不纯度的等等。</p><p>根据所用启发函数的区别，常用的三类决策树为：</p><ol><li>ID3: 使用信息增益</li><li>C4.5: 使用信息增益比，解决ID3对于多取值特征的倾向性。</li><li>CART：使用GINI不纯度</li></ol><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>在建树过程中，根据一些预定义的限制条件进行剪枝，比如树的最大深度、叶子结点的最少样本数、分枝后的增益等等。</p><p>要注意的是，预剪枝有欠拟合风险，虽然当前的划分会导致准确率降低，但在之后的划分中准确率可能会有显著上升。</p><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>后剪枝是在建树完成之后，对完整的决策树进行剪枝，在实际使用中效果一般会优于预剪枝，当然也会更耗时。</p><p>其根据剪枝之后精度损失和复杂度做一个权衡，根据权衡标准选择是否剪枝。</p><p>后剪枝都会损失精度，但是为了提升模型的泛化性，我们也可以在接受一定损失的情况下去做剪枝。</p><p>比如代价复杂度剪枝CCP，思想就是看去掉之后的节点之后，模型损失的精度和精简的复杂度之间的权衡，找一个在一定精度损失的情况下，可精简复杂度最高的。</p><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p><img src="../images/fl3.png"></p><p>核心是将要求的条件概率转换为右边容易求的三个概率，尤其是分子的式子（因为分母是固定的），将其最大化，即bayes分类模型。</p><p>然后，做一个很朴素的假设，假设X的取值是条件独立的，即：</p><p><img src="../images/fl4.png"></p><p>这样的话，可以通过MLE（maximum liklihood estimate,极大似然估计）得到贝叶斯估计的分类结果，若为连续变量，则进行离散化，依旧使用上述方法：</p><p><img src="../images/fl5.png"></p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>基本思想：最大化类别之间的margin</p><p>算法推导思想：将最大化类别之间margin的目标函数使用拉格朗日方法进行转化，使用SMO算法（坐标上升法）对其对偶问题进行求解。</p><p>当数据线性可分时，我们直接使用线性SVM即可，否则可以通过核方法对其进行映射，得到高维空间的线性可分数据。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>直接对分类的概率进行建模，使用MLE进行模型参数的求解即可。</p><p>最后直接就可以通过自变量x得到分类y=k的概率。</p><p><img src="../images/fl6.png"></p><p>z就是一个回归模型，但是回归的并不是概率，而是分类几率的对数，所谓几率就是二分类中两种分类概率的比值，如下所示：</p><p><img src="../images/fl7.png"></p><h2 id="分类模型的评估"><a href="#分类模型的评估" class="headerlink" title="分类模型的评估"></a>分类模型的评估</h2><p>在分类问题中，我们将数据进行如下划分：</p><p><img src="../images/fl10.png"></p><h3 id="precision"><a href="#precision" class="headerlink" title="precision"></a>precision</h3><p>准确率，即所有分为positive的样本中，真正positive的比例。</p><h3 id="recall"><a href="#recall" class="headerlink" title="recall"></a>recall</h3><p>召回率，即所有真正的positive样本中，被正确分为positive的比例。</p><h3 id="f1-score"><a href="#f1-score" class="headerlink" title="f1 score"></a>f1 score</h3><p>准确率和召回率的调和平均数，兼顾二者的表现。</p><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p><img src="../images/fl8.png"></p><h3 id="ROC和AUC"><a href="#ROC和AUC" class="headerlink" title="ROC和AUC"></a>ROC和AUC</h3><p>AUC：一个正例预测为正的概率值大于负例预测为正的概率。</p><p>表现在图像上就是ROC曲线下的面积。</p><p><img src="../images/fl9.png"></p><p>图像上每一个点都对应一个阈值。随着阈值不断减小，TPR和FPR都不断增大，反之不断减小。</p><h3 id="AUC和F1的区别"><a href="#AUC和F1的区别" class="headerlink" title="AUC和F1的区别"></a>AUC和F1的区别</h3><p>二者都对于recall有要求（在AUC中就是TPR），这一点是一致的。</p><p>但是AUC要尽量降低FPR，这要求模型偏向于保守，尽量不误报；<br>但F1要提高准确率，也就是希望不放过任何一个可能的结果。这便是二者的核心区别。</p><p>同时，AUC对不均衡样本并不是很敏感，但是F1很容易在不均衡样本下做出愚蠢的决定。</p><p>总之，AUC是一种较为稳健的评估方式，尤其是在数据失衡的情况下，f1会非常容易受到样本比例的影响，比如正样本占绝大多数时，可能一个愚蠢的模型会将所有的样本划分为正样本，也不会对f1造成多大的影响，但是AUC会敏感的察觉到这种愚蠢的模型。</p>]]></content>
    
    
    <summary type="html">总结一下机器学习领域几种经典的分类算法，包括SVM、KNN、决策树、逻辑回归等等。</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类算法" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类算法" scheme="http://woody5962.github.io/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>集成学习概述</title>
    <link href="http://woody5962.github.io/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"/>
    <id>http://woody5962.github.io/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</id>
    <published>2020-09-09T16:00:00.000Z</published>
    <updated>2021-07-06T08:09:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要用集成学习"><a href="#为什么要用集成学习" class="headerlink" title="为什么要用集成学习"></a>为什么要用集成学习</h2><ol><li>More flexible to interpret</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132442.png"></p><ol start="2"><li>Reduce misclassification rate</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132448.png"></p><h2 id="两个常用的集成学习框架"><a href="#两个常用的集成学习框架" class="headerlink" title="两个常用的集成学习框架"></a>两个常用的集成学习框架</h2><ol><li>Bagging<ul><li>random forests</li></ul></li><li>Boosting<ul><li>Adaboost</li><li>GBDT</li><li>XGBoost</li></ul></li></ol><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>基本思想：通过重置抽样（有放回抽样）在给定的数据集D上抽出m个大小和D一样的数据集，分别用来训练m个分类器，然后将m个分类器的结果通过某种方式形成最终的分类结果。</p><h3 id="自助采样法"><a href="#自助采样法" class="headerlink" title="自助采样法"></a>自助采样法</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132452.png"></p><p>注意：样本集原本有n个样本，就采样n次。这样样本集中会包括63.2%的原始样本（每个样本都有63.2%的概率会在n次采样中被选中），剩下的36.8%用作对泛化性能的包外估计（out-of-bag estimate）</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706132455.png"></p><h3 id="方差分析"><a href="#方差分析" class="headerlink" title="方差分析"></a>方差分析</h3><p>由于多个分类器参与了最终结果的生成，所以方差相对于单个分类器是有所变化的，准确来说，方差要小于单个分类器的方差。</p><p>$$<br>\hat f_{bag}(x) = \frac{1}{M}\sum^M_{m=1}{\hat f_m(x)}<br>$$</p><p>$$<br>\begin{align}<br>Var(\hat f_{bag}(x))=&amp; \frac{1}{M^2}[\sum^M_{m=1}Var(\hat f_m(x))+\sum_{t\neq m}Cov(\hat f_t(x),\hat f_m())]\<br>=&amp; \frac{1}{M^2}[M\sigma^2+M(M-1)\rho(x)\sigma^2]\<br>=&amp; \frac{1}{M}\sigma^2 + \frac{M-1}{M}\rho(x)\sigma^2\<br>=&amp; \rho(x)\sigma^2+\frac{1-\rho(x)}{M}\sigma^2<br>\end{align}<br>$$</p><p>其中，由于$\rho(x)=\frac{Cov(\hat f_t(x),\hat f_m(x))}{\sigma_\theta \cdot \sigma_m}$，而$\sigma_t=\sigma_m=\sigma$<br>$$<br>\begin{align}<br>\rho(x)=\frac{Cov(\hat f_t(x),\hat f_m(x))}{\sigma^2} \<br>Cov(\hat f_t(x),\hat f_m(x)) = \rho(x)\sigma^2<br>\end{align}<br>$$<br>由于在$Var(\hat f_{bag}(x))$计算式中，将$f_t(x)$与$f_m(x)$前后位置做了区分，故一共有$m(m-1)$对。</p><p>在此，暂且将上面的T认为是M。</p><p>可见：</p><ol><li>M越大，方差越小。</li><li>pearson相关系数越小，方差也越小。</li></ol><p>所以我们在bagging中引入random sampling，就是为了减小相关系数。</p><h3 id="decision-tree-and-bagging-tree"><a href="#decision-tree-and-bagging-tree" class="headerlink" title="decision tree and bagging tree"></a>decision tree and bagging tree</h3><p>决策树分两种：分类树和回归树</p><p>其中，分类树以信息论中的启发函数构造决策树，回归树以最小平方误差作为启发函数构造决策树。</p><p>以下是最小二乘回归树的构造算法：</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135239.png"></p><p>其中，c1和c2分别是划分的两个区域对应的y的估计值（也就是输出值），通过最小化估计值与真实值之间的差值来构造决策树，启发信息就是这个差值。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135244.png"><br><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135249.png"></p><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135245.png"></p><ul><li>随机森林就是带随机采样的决策树/回归树森林。</li><li>随机性体现在采样和特征集合选择的随机性</li></ul><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><h4 id="针对数值型输出"><a href="#针对数值型输出" class="headerlink" title="针对数值型输出"></a>针对数值型输出</h4><ol><li>简单平均法</li><li>加权平均法</li></ol><h4 id="针对类别型输出"><a href="#针对类别型输出" class="headerlink" title="针对类别型输出"></a>针对类别型输出</h4><ol><li>绝对多数投票法（某类别超过一半的票）</li><li>相对多数投票法（某类别票数最多）</li><li>加权投票法</li></ol><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>这是一种比较独特的结合策略，通过串行学习来将不同的模型结果结合起来。</p><p>典型代表是stacking，这种结合策略和boosting颇有点神似。</p><p>基本思路是先从初始数据集中训练出初级学习器，然后基于初级学习器结果生成一个新的数据集用于训练次级学习器。</p><ul><li>将每个点的T个初级学习器结果作为输入特征，该点的原始标签作为新数据集的标签得到新数据集。</li><li>在新数据集上使用次级学习算法对次级学习器进行学习。</li></ul><h2 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h2><h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135259.png"></p><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135257.png"></p><ul><li>推导过程见《机器学习》</li><li>其中基分类器权重α_m  是通过添加第m个分类器hm之后最小化指数损失函数得来的</li><li>理想的新分类器hm可以纠正Gm-1的全部错误，基于这样的推导得到了样本分布更新公式，在接受带权样本的算法中直接就是样本的权重，不接受带权样本就直接re-sample。</li><li>理解样本分布更新公式：将所有错分样本的权重增加，使得下一波分类器更加关注这些样本。</li><li>上述算法并不很完善，建议看以下《机器学习》的adaboosting算法及推导（实在懒得做搬运工了）。</li></ul><p>adaboost的思想就是通过不断添加分类器来纠正之前分类器的错误，这是通过在新的样本分布上使用新分类器最小化损失函数得来的。而提升树是很直观的对残差进行建模，不断添加回归树来补充残差。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135303.png"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135305.png"></p><p>Adaboost应该使用指数损失函数或者二项式对数似然损失函数（本质上也使用了指数损失），并且后者效果往往还比指数损失要好一些。</p><h5 id="关于指数损失函数"><a href="#关于指数损失函数" class="headerlink" title="关于指数损失函数"></a>关于指数损失函数</h5><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135307.png"></p><h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><h4 id="boosting-tree"><a href="#boosting-tree" class="headerlink" title="boosting tree"></a>boosting tree</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135310.png"></p><p>提升树包括梯度提升树都是通过残差拟合进行boosting的，与标准的boosting相比，权重更新体现在残差越大的样本，其在回归树中的结果就越大，只不过标准的boosting还需要将更新权重的样本重新进行训练，而提升树直接就得到了相应的叶节点区域，直接影响了结果，直接在上一轮学习器结果的基础上，加上残差进行校正，不停的迭代，学习器残差就会越来越小。</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135312.png"><br><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706144857.png"></p><h4 id="算法解析"><a href="#算法解析" class="headerlink" title="算法解析"></a>算法解析</h4><ol><li>初始化就是求一个只有一个叶节点区域的树，对于回归树而言，就是所有的样本点都拟合成一个值。</li><li>使用负梯度代替残差，这样更加通用。</li><li>生成新的数据集（xi, rmi）作为生成第m棵回归树的训练数据，得到j个叶节点区域，对每一个叶节点区域计算回归目标值，也就是得到j个回归目标值。</li><li>将j个针对残差的回归目标值乘以示性函数加到之前得到的提升树结果中。</li><li>最终的回归树用法：得到x在M棵回归树中的叶节点区域目标值，进行累加，其实就是一步一步的将残差进行补充。</li><li>提升树类算法就是通过不断向结果中添加残差来对结果进行校正。</li></ol><h4 id="正则化技巧"><a href="#正则化技巧" class="headerlink" title="正则化技巧"></a>正则化技巧</h4><ol><li>shrinkage ：每次更新，在残差拟合结果上乘以正则化系数</li><li>下采样： 每次更新，都是在训练集的一个子集上进行的</li></ol><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135319.png"></p><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706135323.png"></p><p>$$<br>L=\sum^{n}<em>{i=1}l(y_i,\hat y_i)+\sum_k\Omega(f_k)<br>$$<br>其中，第t棵树生成：<br>$$<br>\begin{align}<br>L^{(t)}=&amp;\sum^{n}</em>{i=1}l(y_i,y_i^{(t)})+\Omega(f_t)\<br>=&amp;\sum^{n}<em>{i=1}l(y_i,y_i^{(t-1)}+f_t(x_i))+\Omega(f_t)\<br>二阶泰勒展开\Rightarrow\ \ \simeq&amp;\sum^n</em>{i=1}[l(y_i,y_i^{(t-1)})+g_if_t(x_i)+\frac{h_i}{2}f^2_t(x_i)]+\Omega(f_t)<br>\end{align}<br>$$<br>由于$l(y_i,y_i^{(t-1)})$对$f_t$的优化无影响</p><p>另$L^(t)=\sum^n_{i-1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)$</p><p>将同一叶节点区域的样本点合并<br>$$<br>\begin{align}<br>L^{(t)}=&amp;\sum^T_{j=1}[(\sum_{i\in I_j}g_i)f_i(x)+\frac{1}{2}(\sum_{i\in I_j}h_i)f_j^2(x)]+\gamma T+\frac{\lambda}{2}\sum^T_{j=1}f_j^2(x)\<br>=&amp;\sum^T_{j=1}[(\sum_{i\in I_j}g_i)f_i(x)+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)f_j^2(x)]+\gamma T<br>\end{align}<br>$$<br>另$G_j=\sum_{i\in Ij}g_j \ \ \ \ \ H_j=\sum_{i\in I_j}h_i$<br>$$<br>L^{(t)}=\sum^T_{j=1}[G_jf_j(x)+\frac{1}{2}(H_jj+\lambda)f_j^2(x)]+\gamma T<br>$$<br>构造出关于$f_i(x)$的一元二次方程，解得<br>$$<br>\begin{align}<br>f_j(x)=&amp;-\frac{G_j}{H_j+\lambda}\<br>\hat L^{(t)}=&amp;-\frac{1}{2}\sum^T_{j=1}\frac{G_j^2}{H_j+\lambda}+\gamma T<br>\end{align}<br>$$<br>其中，$g_i=\frac{\partial l(y_i,y_i^{t-1})}{\partial ly_i^{t-1}}\ \ \ \ \ \ \ \ h_i=\frac{\partial^2 l(y_i,y_i^{t-1})}{(\partial y_i^(t-1))^2}$已知</p><p>新的$T$个叶子节点值可由$f_j(x)=\frac{G_j}{H_j+\lambda}$得到。</p><ul><li>简而言之，GBDT使用最速下降法，这里使用牛顿法进行优化。</li><li>同样的，都是在已有的学习器基础上添加新的学习器以校正其效果，根据公式可以看到GBDT类似于最速下降法优化学习器，而XGBoost类似于牛顿法优化学习器，二者都是对学习器结果的直接校正，某些情况下可以认为是残差，但是准确来说，就是损失函数关于学习器的负梯度变形。</li></ul><p>ps：牛顿法</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142506.png"></p><h4 id="分裂准则"><a href="#分裂准则" class="headerlink" title="分裂准则"></a>分裂准则</h4><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142504.png"><br><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142501.png"></p><p>综合以上，就是利用得到的目标函数进行启发式分支，然后得到的叶子节点的值有公式给出。</p><h4 id="缺失值的处理"><a href="#缺失值的处理" class="headerlink" title="缺失值的处理"></a>缺失值的处理</h4><p>在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。</p><p>在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。</p><p>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142458.png"><br><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142455.png"></p><ul><li>可以看到，最外层遍历k从1到m是在遍历所有m个特征。</li><li>内层是将第k个特征所有无缺失值的样本按照特征k的值进行排序，然后遍历分裂，寻找最优分裂点。</li><li>以第一个内层for循环为例，先指定了left的G和H，然后用全集的G和L减去了Left的，所有k特征缺失的点就被划分到了right子树。</li></ul><h4 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h4><p>对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。</p><p>因此XGBoost思想是对特征进行分桶，即找到l个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。</p><p>从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。</p><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142453.png"></p><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>XGBoost还提出了两种防止过拟合的方法：</p><ol><li>Shrinkage</li><li>Column Subsampling。</li></ol><p>Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。</p><p>Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，</p><ul><li>一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。</li><li>另一种是随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。</li></ul><p>一般情况下前者效果更好。</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>之所以XGBoost可以成为机器学习的大杀器，广泛用于数据科学竞赛和工业界，是因为它有许多优点：</p><ol><li>使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。</li><li>目标函数优化利用了损失函数关于待求函数的二阶导数</li><li>支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。</li><li>添加了对稀疏数据的处理。</li><li>交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。</li><li>支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://gitee.com/halfcoke/blog_img/raw/master/20210706142449.png"></p><p>在这里把boosting框架分个类：</p><ul><li>adaboost是一类，这一类通过更新样本权重来训练新的分类器</li><li>还有一类是boosting tree、GBDT和xgboost，通过梯度或者残差来不断修正结果。</li></ul>]]></content>
    
    
    <summary type="html">常用的集成算法总结与分析</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="集成学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="集成学习" scheme="http://woody5962.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>元学习简介</title>
    <link href="http://woody5962.github.io/%E5%85%83%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>http://woody5962.github.io/%E5%85%83%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/</id>
    <published>2020-06-09T16:00:00.000Z</published>
    <updated>2021-07-15T16:13:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>说白了，就是学习一个meta- knowledge，使用这个knowledge去快速适应新的任务。</p><p>即，学习怎么去学习。</p><h2 id="与传统机器学习的区别"><a href="#与传统机器学习的区别" class="headerlink" title="与传统机器学习的区别"></a>与传统机器学习的区别</h2><p>这里学习的目标是一个meta-knowledge，feed的不是数据而是任务，分为train task和test task，而每一个task内部有训练数据和测试数据，分别叫做support set和query set。相对应的，在测试中输出的并不是对support set的计算结果，而是一个针对测试task的f，在测试task的query set上可以得到f（data）</p><p><img src="../images/ml1.png"></p><h2 id="与迁移学习的区别"><a href="#与迁移学习的区别" class="headerlink" title="与迁移学习的区别"></a>与迁移学习的区别</h2><ol><li>这里有专门的meta-net，对不同的task会专门使用meta-net初始化一套specific task net，在task net中更新参数，然后利用更新之后的参数进行损失计算并回传更新（可以1步，可以k步，步数越少越符合我们的使用目标，就是可以使meta-net生成的模型更快的适应task），此时再对meta-net中的参数进行同方向的更新，目的是使训练过程和使用目的一致，而使用目的正是能够对新的任务生成f，f能够快速适应该任务，在之后产生不错的效果，但并不是追求当下的效果。</li></ol><p>下图分别展示了maml和reptile的meta-learning框架：</p><p><img src="../images/ml2.png"><br><img src="../images/ml3.png"></p><ol start="2"><li>迁移学习使用同一套参数，并不会针对专门的任务另外初始化一套网络，每一个任务的损失都直接影响model param，目的是得到当下的最优，也就是针对新任务可以直接有较优的结果。</li></ol><p><img src="../images/ml4.png"><br><img src="../images/ml5.png"></p>]]></content>
    
    
    <summary type="html">元学习简介及其与传统机器学习、迁移学习的区别</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="元学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="元学习" scheme="http://woody5962.github.io/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理</title>
    <link href="http://woody5962.github.io/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://woody5962.github.io/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</id>
    <published>2020-06-09T16:00:00.000Z</published>
    <updated>2022-10-16T08:47:12.817Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="表格数据"><a href="#表格数据" class="headerlink" title="表格数据"></a>表格数据</h3><p>行为对象（元组、数据点、实例、样本均为相同概念），列为属性</p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>维、特征、变量、属性均为相同概念</p><h3 id="数据类型-1"><a href="#数据类型-1" class="headerlink" title="数据类型"></a>数据类型</h3><ul><li>名义型</li><li>布尔型</li><li>等级型</li><li>数值型</li></ul><h3 id="数据的基本统计信息"><a href="#数据的基本统计信息" class="headerlink" title="数据的基本统计信息"></a>数据的基本统计信息</h3><p>统计中心趋势和离散程度：</p><ul><li>均值：算数均值、加权均值</li><li>中位数（median）</li><li>众数（mode）</li><li>对于适度倾斜的单峰曲线，有经验公式：mean - mode = 3 * (mean - median)</li></ul><p><img src="../images/cl1.png"></p><ul><li>分位数（Quantile）：x为数据集中的数，如果数据集中百分之K的数都不大于x，则称x为数据集的K分位数</li><li>上四分位数（Q3-75% quentile）</li><li>下四分位数（Q1-25% quentile）</li><li>中间分位数（IQR）: IQR = Q3 - Q1, 也叫四分位距，用来衡量数据的分散情况。</li><li>方差（Variance）和标准差（Standard deviation）</li></ul><p><img src="../images/cl2.png"></p><ul><li>盒图（箱线图）</li></ul><p><img src="../images/cl3.png"></p><h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><p>相似性，值域通常为[0,1]</p><h3 id="名义型数据类型-amp-布尔型数据类型"><a href="#名义型数据类型-amp-布尔型数据类型" class="headerlink" title="名义型数据类型 &amp; 布尔型数据类型"></a>名义型数据类型 &amp; 布尔型数据类型</h3><ul><li>简单匹配（相同属性个数m，属性数量p，则距离为p-m/p）</li><li>转换为多个布尔属性，如下图所示</li></ul><p><img src="../images/cl4.png"></p><p>q是两者都有的一个属性，r是i有但是j没有的属性。</p><h3 id="数值型数据类型"><a href="#数值型数据类型" class="headerlink" title="数值型数据类型"></a>数值型数据类型</h3><p>明可夫斯基距离：</p><p><img src="../images/cl5.png"></p><p>余弦相似度：</p><p><img src="../images/cl6.png"></p><p>前者是绝对距离，后者是方向差异。</p><p>前者体现维度数值大小的差异，后者体现类似于用户兴趣之类的差异。</p><h3 id="等级型数值类型"><a href="#等级型数值类型" class="headerlink" title="等级型数值类型"></a>等级型数值类型</h3><p><img src="../images/cl7.png"></p><h3 id="混合型数据类型"><a href="#混合型数据类型" class="headerlink" title="混合型数据类型"></a>混合型数据类型</h3><p>将上述的类型求得的距离进行加权求和。</p><h2 id="数据预处理的方式"><a href="#数据预处理的方式" class="headerlink" title="数据预处理的方式"></a>数据预处理的方式</h2><ul><li>数据清洗</li><li>数据集成</li><li>数据转换</li><li>数据归约</li></ul><p><img src="../images/cl8.png"></p><h3 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h3><p>算法要求或者数据量纲不同：</p><ol><li>Z-score标准化：接近（0，1）分布，适用于最值未知</li><li>0-1标准化：线性变换到[0,1]区间</li><li>小数定标标准化：</li></ol><p><img src="../images/cl9.png"></p><ol start="4"><li>logistic标准化</li></ol><p><img src="../images/cl10.png"></p><p>当数据集中的分布在零点附近时，通过sigmoid函数可以将其均匀地散开。</p><h4 id="方法比较"><a href="#方法比较" class="headerlink" title="方法比较"></a>方法比较</h4><p><img src="../images/cl11.png"></p><h3 id="数据离散化"><a href="#数据离散化" class="headerlink" title="数据离散化"></a>数据离散化</h3><p>有些数据挖掘算法，特别是某些分类算法（如朴素贝叶斯），要求数据是分类属性形式（类别型属性）这样常常需要将连续属性变换成分类属性（离散化，Discretization）。另外，如果一个分类属性（或特征）具有大量不同值，或者某些之出现不频繁，则对于某些数据挖掘任务，通过合并某些值减少类别的数目可能是有益的。(具体可参考“离散化与稠密化”)</p><p>本质：将连续型数据分段</p><h4 id="无监督离散化"><a href="#无监督离散化" class="headerlink" title="无监督离散化"></a>无监督离散化</h4><ul><li>等距离散化</li><li>等频离散化</li><li>基于聚类分析的离散化（先聚类，再合并（自底向上）或分裂（自顶向下），达到预定的簇个数）</li><li>基于正态3σ的离散化</li><li>有监督离散化</li><li>基于信息增益的离散化（自顶向下）</li><li>基于卡方的离散化（自底向上，根据不同区间的类分布相似度进行聚合，相似度越高，卡方值越小）</li></ul><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><img src="../images/cl12.png"></p><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h3 id="缺失机制"><a href="#缺失机制" class="headerlink" title="缺失机制"></a>缺失机制</h3><ul><li>完全随机缺失</li><li>随机缺失（与完全变量有关）</li><li>完全非随机缺失（与不完全变量自身有关）</li></ul><h3 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h3><h4 id="删除法"><a href="#删除法" class="headerlink" title="删除法"></a>删除法</h4><ul><li>删除样本</li><li>删除变量（缺得多，对研究目标影响不大）</li><li>降低权重（不影响数据结构）</li></ul><h4 id="基于填补的方法"><a href="#基于填补的方法" class="headerlink" title="基于填补的方法"></a>基于填补的方法</h4><ul><li>单一填补（均值、k-means、热平台、冷平台）</li><li>随机填补（在均值填补的基础上加随机项）</li><li>基于模型的方法</li><li>建模预测</li></ul><h2 id="异常值检测"><a href="#异常值检测" class="headerlink" title="异常值检测"></a>异常值检测</h2><h3 id="基于统计的方法"><a href="#基于统计的方法" class="headerlink" title="基于统计的方法"></a>基于统计的方法</h3><p>上下α分位数之外的值</p><h3 id="基于距离的算法"><a href="#基于距离的算法" class="headerlink" title="基于距离的算法"></a>基于距离的算法</h3><p>局部异常因子算法（LOF算法）</p>]]></content>
    
    
    <summary type="html">机器学习中的数据预处理简介</summary>
    
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://woody5962.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="ETL" scheme="http://woody5962.github.io/tags/ETL/"/>
    
  </entry>
  
  <entry>
    <title>集束搜索</title>
    <link href="http://woody5962.github.io/%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2/"/>
    <id>http://woody5962.github.io/%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2/</id>
    <published>2019-09-23T16:00:00.000Z</published>
    <updated>2021-07-12T04:04:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h2><p>举例说明，在NLP领域中，经常会涉及机器翻译结果的搜索，比如在decoder中，我们可以使用贪心策略每次选择softmax值最高的，但是由于贪心解往往达不到最优，所以我们放松贪心约束，每次选择最优的k个，k便是集束宽度，这样对解的搜索叫做集束搜索。</p><h2 id="过程示例"><a href="#过程示例" class="headerlink" title="过程示例"></a>过程示例</h2><p>假设字典为[a,b,c]，beam size选择2，则如下图有：</p><p><img src="../images/jishu1.png"></p><ol><li><p>在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b</p></li><li><p>生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb</p></li></ol><p>不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。</p><p>显然集束搜索属于贪心算法，不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。</p><p>而贪心搜索由于每次考虑当下词的概率，而通常英文中有些常用结构，比如吴恩达网课中的例子：“is going”，出现概率较大，会导致模型最终生成的句子过于冗余，如“is visiting”和“is going to be visiting”。</p><p>贪心搜索可以认为beam size为1时的集束搜索特例。</p>]]></content>
    
    
    <summary type="html">集束搜索的思想和算法步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>模拟退火算法</title>
    <link href="http://woody5962.github.io/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-21T16:00:00.000Z</published>
    <updated>2022-10-16T08:52:15.026Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这个方法来自金属热加工过程的启发。在金属热加工过程中，当金属的温度超过它的熔点（Melting Point）时，原子就会激烈地随机运动。与所有的其它的物理系统相类似，原子的这种运动趋向于寻找其能量的极小状态。在这个能量的变迁过程中，开始时，温度非常高， 使得原子具有很高的能量。随着温度不断降低，金属逐渐冷却，金属中的原子的能量就越来越小，最后达到所有可能的最低点。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>利用模拟退火的时候，让算法从较大的跳跃开始，使到它有足够的“能量”逃离可能“路过”的局部最优解而不至于限制在其中，当它停在全局最优解附近的时候，逐渐的减小跳跃量，以便使其“落脚 ”到全局最优解上。</p><h2 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h2><p>从一个问题的原始解开始，用一个变量代表温度，这一温度开始时非常高，而后逐步减低。在每一次迭代期间，算法会随机选中题解中的某个数字，使其发生细微变化，而后计算该解的代价。关键的地方在于计算出该解的代价后，如何决定是否接受该解。 如果新的成本更低，则新的题解就会变成当前题解，这与爬山法类似；如果新的成本更高，则新的题解与概率 P 被接受（一开始的时候，T很高，导致P很大，于是就有了足够的能量逃离局部最优）。这一概率会随着温度T的降低而降低。即算法开始时，可以接受表现较差的解，随着退火过程中温度的不断下降，算法越来越不可以接受较差的解，知道最后，它只会接受更优的解。 其中P = exp[-（newcost - oldcost）/ T ] 其中newcost是新解的成本，oldcost是当前成本，T为当前温度。算法以概率P接受新的解。可见，当温度很高或者新旧解差别很大时，概率P都会很大。</p>]]></content>
    
    
    <summary type="html">模拟退火算法的思想和算法步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="控制算法" scheme="http://woody5962.github.io/tags/%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>蚁群算法</title>
    <link href="http://woody5962.github.io/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-20T16:00:00.000Z</published>
    <updated>2021-07-12T04:05:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h2><p>将蚁群算法应用于解决优化问题的基本思路为：用蚂蚁的行走路径表示待优化问题的可行解，整个蚂蚁群体的所有路径构成待优化问题的解空间。路径较短的蚂蚁释放的信息素量较多，随着时间的推进，较短的路径上累积的信息素浓度逐渐增高，选择该路径的蚂蚁个数也愈来愈多。最终，整个蚂蚁会在正反馈的作用下集中到最佳的路径上，此时对应的便是待优化问题的最优解。</p><h2 id="算法流程（蚁群解决TSP）"><a href="#算法流程（蚁群解决TSP）" class="headerlink" title="算法流程（蚁群解决TSP）"></a>算法流程（蚁群解决TSP）</h2><p><img src="../images/yiqun1.jpg"></p><h3 id="选择下一座城市的规则"><a href="#选择下一座城市的规则" class="headerlink" title="选择下一座城市的规则"></a>选择下一座城市的规则</h3><p><img src="../images/yiqun2.png"></p><ul><li><p>由选择公式可见，分母是所有可选路径的信息素的和，而某路径信息素越浓，选中它的概率越大。</p></li><li><p>同时，给出了beta参数作为启发信息的权重，在这里，启发信息正如其名，是一种启发，根据其初值可知，哪条路径最短，其被选中的概率就越高。在早期，就是这样的启发信息来促使算法少走弯路。</p></li></ul><h3 id="信息素更新规则"><a href="#信息素更新规则" class="headerlink" title="信息素更新规则"></a>信息素更新规则</h3><p><img src="../images/yiqun3.png"></p><p><img src="../images/yiqun4.png"></p><ul><li><p>当Lk 计算完毕之后，得到目前最优解，根据最优解的路径更新信息素。</p></li><li><p>可见，信息素在更新之前，先削弱了旧信息素的影响，然后将最优路径的信息素添加进去，由于最优，所以L小，信息素大。</p></li></ul><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>实际实验中发现，当蚂蚁在一条路径上觅食很久时，放置一个近的食物基本没有效果，这可以理解为当一只蚂蚁找到一条路径时，在经过很长时间后大多数蚂蚁都选择了这条路径，这时，突然有一只蚂蚁找到了较近的食物，但因为时间过得太久，两条路径上浓度相差太大（浓度越大，被选择的概率就越大），整个系统基本已经停滞了，陷入了局部最优。</p>]]></content>
    
    
    <summary type="html">蚁群算法思想和步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="蚁群算法" scheme="http://woody5962.github.io/tags/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>随机搜索算法</title>
    <link href="http://woody5962.github.io/%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-19T16:00:00.000Z</published>
    <updated>2021-07-01T10:55:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于该算法"><a href="#关于该算法" class="headerlink" title="关于该算法"></a>关于该算法</h2><p>随机搜索算法时最简单的优化搜索算法。</p><p>类似于调参过程中的随机搜索，当不知道代价函数在值域范围内的变化规律时，可以使用随机算法对解进行搜索。</p><h2 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h2><p>只适用于代价函数在值域范围内没有任何变化规律的情况，即找不到任何使得代价下降的梯度和极小值点。</p><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>只需要在值域范围内生成足够多的可行解，然后分别计算每个可行解的代价，根据代价选择一个最小的可行解作为随机搜索的最优解即可。</p>]]></content>
    
    
    <summary type="html">随机搜索的简介</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>禁忌搜索算法</title>
    <link href="http://woody5962.github.io/%E7%A6%81%E5%BF%8C%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E7%A6%81%E5%BF%8C%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-18T16:00:00.000Z</published>
    <updated>2021-07-12T04:04:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h2><p><img src="../images/jinji1.png"><br>通过保持一个禁忌表，以禁忌长度进行禁忌，遵守禁忌准则和藐视准则进行全局搜索。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>首先，我们对置换问题定义一种邻域搜索结构，如互换操作（SWAP），即随机交换两个点的位置，则每个状态的邻域解有Cn2=n（n一1）/2个。称从一个状态转移到其邻域中的另一个状态为一次移动（move），显然每次移动将导致适配值（反比于目标函数值）的变化。其次，我们采用一个存储结构来区分移动的属性，即是否为禁忌“对象”在以下示例中：考虑7元素的置换问题，并用每一状态的相应21个邻域解中最优的5次移动（对应最佳的5个适配值）作为候选解；为一定程度上防止迂回搜索，每个被采纳的移动在禁忌表中将滞留3步（即禁忌长度），即将移动在以下连续3步搜索中将被视为禁忌对象；需要指出的是，由于当前的禁忌对象对应状态的适配值可能很好，因此在算法中设置判断，若禁忌对象对应的适配值优于“ best so far”状态，则无视其禁忌属性而仍采纳其为当前选择，也就是通常所说的藐视准则（或称特赦准则）。<br>注意，出于改善算法的优化时间性能的考虑，若领域结构决定了大量的领域解（尤其对大规模问题，如TSP的SWAP操作将产生Cn2个领域解），则可以仅尝试部分互换的结果，而候选解也仅取其中的少量最佳状态。</p><h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>简单TS算法的基本思想是：给定一个当前解（初始解）和一种邻域，然后在当前解的邻域中确定若干候选解；若最佳候选解对应的目标值优于“best so far”状态，则忽视其禁忌特性，用其替代当前解和“best so far”状态，并将相应的对象加入禁忌表，同时修改禁忌表中各对象的任期；若不存在上述候选解，则选择在候选解中选择非禁忌的最佳状态为新的当前解，而无视它与当前解的优劣，同时将相应的对象加入禁忌表，并修改禁忌表中各对象的任期；如此重复上述迭代搜索过程，直至满足停止准则。</p><p>　　条理化些，则简单禁忌搜索的算法步骤可描述如下：</p><p>　　（1）给定算法参数，随机产生初始解x，置禁忌表为空。</p><p>　　（2）判断算法终止条件是否满足？若是，则结束算法并输出优化结果；否则，继续以下步骤。</p><p>　　（3）利用当前解工的邻域函数产生其所有（或若干）邻域解，并从中确定若干候选解。</p><p>　　（4）对候选解判断藐视准则是否满足？若成立，则用满足藐视准则的最佳状态y替代x成为新的当前解，即x=y，并用与y对应的禁忌对象替换最早进入禁忌表的禁忌对象，同时用y替换“best so far”状态，然后转步骤6；否则，继续以下步骤。</p><p>　　（5）判断候选解对应的各对象的禁忌属性，选择候选解集中非禁忌对象对应的最佳状态为新的当前解，同时用与之对应的禁忌对象替换最早进入禁忌表的禁忌对象元素。</p><p>　　（6）转步骤（2）。</p><p>注意，根据（5）可知，最牛逼的解根本就不考虑它是不是在禁忌列表中，如果这些候选解都很弟弟，那就找非禁忌里面最优的了。这样，既可以保证不遗漏全局最优，又有利于跳出局部最优。</p>]]></content>
    
    
    <summary type="html">禁忌搜索的算法思想和步骤</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="搜索算法" scheme="http://woody5962.github.io/tags/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>粒子群优化算法（PSO）</title>
    <link href="http://woody5962.github.io/%E7%B2%92%E5%AD%90%E7%BE%A4%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E7%B2%92%E5%AD%90%E7%BE%A4%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-18T16:00:00.000Z</published>
    <updated>2022-10-16T08:56:09.143Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>假设有一群鸟，在随机搜索食物，在搜索区域内只有一块儿食物，一开始时所有的鸟儿都不知道食物所在的方位，但它们能够知道自己离食物有多远，以及它们能够记住在自己飞过的路程当中距离食物最近的位置，同时它们也能够知道鸟群中所有鸟儿经过的路程当中，离食物最近的位置。那每一只鸟儿将如何去寻找食物呢？简单来说，每一只鸟儿在当前位置的基础上，如何做出决策，下一步向哪里飞呢？实际，每只鸟儿将综合自身的经验，以及群体的经验来在做出下一步飞向哪里的决策，即每只鸟儿将根据自己所经过的路程中离食物最近的位置以及鸟群中所有鸟儿经过的路程当中离食物最近的位置来做出决策，决定下一步自己向哪里飞。</p><p>在粒子群算法中，粒子的位置对应于原问题的解。粒子的适应值就是将粒子的位置（对应于原问题的解）带入到目标函数中所得到的目标函数值。粒子的速度决定粒子下一步向哪里飞以及飞多远。</p><h2 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h2><p><img src="../images/pso1.png"></p><h3 id="参数解释"><a href="#参数解释" class="headerlink" title="参数解释"></a>参数解释</h3><p><img src="../images/pso2.png"></p><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>粒子群算法整合了个体与整体的智慧，在“信息素”更新的时候，综合考虑了个体和整体在过去的发现。调整参数可以避免陷入局部最优，同时也可以决定对个体或者整体的依赖程度。个人认为，整体越大，对整体的依赖程度就要越大，这个依赖程度应该和粒子数量成正相关关系。</p>]]></content>
    
    
    <summary type="html">粒子群优化算法的背景、原理以及一些个人理解</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>进化算法</title>
    <link href="http://woody5962.github.io/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-17T16:00:00.000Z</published>
    <updated>2021-06-30T16:57:38.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>进化算法主要包括：</p><ul><li>遗传算法</li><li>进化程序设计</li><li>进化规划</li><li>进化策略</li></ul><p>之后的文章记录遗传算法、进化规划、进化策略三种算法的理解。</p><h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><h3 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h3><p>适者生存，优胜劣汰！</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="概念0：编码与解码"><a href="#概念0：编码与解码" class="headerlink" title="概念0：编码与解码"></a>概念0：编码与解码</h4><p>要将问题进行数学建模，将问题的解利用二进制数字串进行表示。<br>而进行适应度分析时，要将其解码成特征进行评价。</p><h4 id="概念1：基因和染色体"><a href="#概念1：基因和染色体" class="headerlink" title="概念1：基因和染色体"></a>概念1：基因和染色体</h4><p>在遗传算法中，我们首先需要将要解决的问题映射成一个数学问题，也就是所谓的“数学建模”，那么这个问题的一个可行解即被称为一条“染色体”。一个可行解一般由多个元素构成，那么这每一个元素就被称为染色体上的一个“基因”。</p><h4 id="概念2：适应度函数"><a href="#概念2：适应度函数" class="headerlink" title="概念2：适应度函数"></a>概念2：适应度函数</h4><p>给染色体打分（评价解的优良程度）</p><h4 id="概念3：交叉"><a href="#概念3：交叉" class="headerlink" title="概念3：交叉"></a>概念3：交叉</h4><p>遗传算法每一次迭代都会生成N条染色体，在遗传算法中，这每一次迭代就被称为一次“进化”。交叉算子种类很多，具体使用时找收藏夹，要注意有的交叉算子处理之后要进行冲突检查。</p><h4 id="概念4：变异"><a href="#概念4：变异" class="headerlink" title="概念4：变异"></a>概念4：变异</h4><p>当我们通过交叉生成了一条新的染色体后，需要在新染色体上随机选择若干个基因，然后随机修改基因的值，从而给现有的染色体引入了新的基因，突破了当前搜索的限制，更有利于算法寻找到全局最优解。交叉只能找到局部最优解，永远没有办法达到全局最优解。</p><h4 id="概念5：复制"><a href="#概念5：复制" class="headerlink" title="概念5：复制"></a>概念5：复制</h4><p>每次进化中，为了保留上一代优良的染色体，需要将上一代中适应度最高的几条染色体直接原封不动地复制给下一代。假设每次进化都需生成N条染色体，那么每次进化中，通过交叉方式需要生成N-M条染色体，剩余的M条染色体通过复制上一代适应度最高的M条染色体而来。</p><h4 id="概念6：选择"><a href="#概念6：选择" class="headerlink" title="概念6：选择"></a>概念6：选择</h4><p>如何从群体中找到染色体进行交叉，这涉及到一定的选择策略，常用的有轮盘赌选择策略、锦标赛选择策略、线性排序选择（Linear Ranking Selection）和指数排序选择（Exponential Ranking Selection）。后面两种选择策略避免了轮盘赌的一个缺点，那就是可以让适应度为0的个体也有机会产生后代（会确定一个最低概率）。</p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ol><li>在算法初始阶段，它会随机生成一组可行解，也就是第一代染色体。</li><li>采用适应度函数分别计算每一条染色体的适应程度，并根据适应程度计算每一条染色体在下一次进化中被选中的概率(这个上面已经介绍，这里不再赘述)。<br>（上面都是准备过程，下面正式进入“进化”过程)</li><li>通过“交叉”，生成N-M条染色体；</li><li>对交叉后生成的N-M条染色体进行“变异”操作；</li><li>使用“复制”的方式生成M条染色体；</li><li>N条染色体生成完毕,紧接着分别计算N条染色体的适应度和下次被选中的概率。<br>(这就是一次进化的过程，紧接着进行新一轮的进化)</li></ol><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>进化次数允许范围：算法到达一定效果就停止，保证效率。</p><h2 id="进化策略"><a href="#进化策略" class="headerlink" title="进化策略"></a>进化策略</h2><h3 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h3><ol><li>编码：对要求解的问题以数字串的方式进行编码（由目标参数和策略参数组成），计算合适度值</li><li>判断是否满足终止条件：如满足则输出结果；否则执行下述步骤</li><li>选择n个父代参与繁殖</li><li>按给定的方式执行交叉操作（可选）</li><li>基于高斯分布的扰动执行变异操作</li><li>产生m个子代，并计算合适度值（m&gt;n）</li><li>返回步骤2</li></ol><h2 id="进化规划"><a href="#进化规划" class="headerlink" title="进化规划"></a>进化规划</h2><ol><li>编码：对要求解的问题以数字串的方式进行编码（由目标参数和策略参数组成），计算合适度值</li><li>判断是否满足终止条件：如满足则输出结果；否则执行下述步骤</li><li>选择n个父代参与繁殖</li><li>基于高斯分布的扰动执行变异操作</li><li>产生n个子代，并计算合适度值</li><li>返回步骤2</li></ol><h2 id="进化策略、进化规划与遗传算法"><a href="#进化策略、进化规划与遗传算法" class="headerlink" title="进化策略、进化规划与遗传算法"></a>进化策略、进化规划与遗传算法</h2><ol><li><p>编码方面：不像传统遗传算法那样需要对要求解的问题进行0-1编码和解码，而是直接对求解的问题进行编码，即直接将优化问题的解表示为数字串的形式，不需要特定的编码和译码，可直接进行实数编码。</p></li><li><p>均采用同样的变异操作方式，即变异时，对父代中的个体加上一个服从均值为0，标准差为 σ 的高斯分布随机变量。标准差是变化的，编码时属于染色体串中的一部分。</p></li></ol><h2 id="进化策略与进化规划的差别"><a href="#进化策略与进化规划的差别" class="headerlink" title="进化策略与进化规划的差别"></a>进化策略与进化规划的差别</h2><ol><li>进化策略中的交叉算子是可选的；如需要进行交叉运算时，采用类似遗传算法的处理方法，如离散交叉或中值交叉方式。进化规划没有交叉算子。</li><li>父代选择方面：进化策略采用概率选择的方法形成父代（如基于随机分布的方式抽取父代个体），父代每一个个体都能以同样的概率被选中。 进化规划采用确定性方式，即当前种群中每个父代都要经过变异来产生子代。</li><li>变异表达式上的差异。</li><li>生存选择方面，即新的子代生成后，如何和父代共同形成新的父代的方式。</li></ol>]]></content>
    
    
    <summary type="html">进化算法主要涵盖的算法，包括其中各种算法的区别和联系</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    <category term="进化算法" scheme="http://woody5962.github.io/tags/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>启发式算法概述</title>
    <link href="http://woody5962.github.io/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    <id>http://woody5962.github.io/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/</id>
    <published>2019-09-09T16:00:00.000Z</published>
    <updated>2022-10-16T09:07:35.382Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>一个基于直观或经验构造的算法，在可接受的花费（计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。</p><p>现阶段，启发式算法以仿自然体算法为主，主要有蚁群算法、模拟退火法、神经网络等。</p><p>个人理解，所谓启发就是，没有精确的数学公式得到直接解的情况下，告诉机器该如何去发现指导信息，进而根据指导信息去寻找解。</p><h2 id="元启发式算法"><a href="#元启发式算法" class="headerlink" title="元启发式算法"></a>元启发式算法</h2><ul><li><a href="https://woody5962.github.io/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/">模拟退火算法</a></li><li>遗传算法</li><li>列表搜索算法</li><li>进化规划</li><li><a href="https://woody5962.github.io/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/">进化算法</a><ul><li>进化策略</li><li>蚁群算法</li></ul></li><li>人工神经网络</li><li><a href="https://woody5962.github.io/%E7%A6%81%E5%BF%8C%E6%90%9C%E7%B4%A2/">禁忌搜索</a></li><li><a href="https://woody5962.github.io/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/">粒子群优化</a></li></ul><h2 id="超启发式算法"><a href="#超启发式算法" class="headerlink" title="超启发式算法"></a>超启发式算法</h2><ul><li>基于随机选择的超启发式算法</li><li>基于贪心策略的超启发式算法</li><li>基于元启发式算法的超启发式算法</li><li>基于学习的超启发式算法</li></ul>]]></content>
    
    
    <summary type="html">启发式算法的定义和分类，以及一些个人的理解</summary>
    
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/categories/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="启发式算法" scheme="http://woody5962.github.io/tags/%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
